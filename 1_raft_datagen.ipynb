{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RAFT training data generation using GPT-4o\n",
    "\n",
    "In this notebook, we'll synthesize some training data that will eventually be used to fine-tune a GPT-4o mini model in order to adapt it to a set of document(s) and specific domain. **This step is critical as the quality of your training data will greatly influence the performance of your fine-tuned model.**\n",
    "\n",
    "For RAFT, these are the different steps to prepare the training dataset:\n",
    "- Collect Domain-Specific Documents: Gather documents relevant to the domain you want to specialize the LLM in (e.g., medical documents for PubMed, legal documents, API documentation for software).\n",
    "- Chunk the file into Documents\n",
    "- For each Document chunk, generate a set of Questions that can be answered from the Document\n",
    "- For each Document-Question pair, create a list of documents using:\n",
    "    - **Golden Document (D*)**: Document that contains the answer to the question.\n",
    "    - **Distractor Documents (Dk)**: Documents that do not contain relevant information.\n",
    "- Question-Answer-Document Triplets: From each **Document-Question** pair, generate a factual **Answer** based on the Golden Document.\n",
    "\n",
    "Curating a good training dataset often involves manual work and review by SMEs. That said, we can use an LLM to help us generate an initial set of training examples that can be vetted and further refined by SMEs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Pre-requisites\n",
    "\n",
    "For this hands-on workshop, all you need is access to an Azure subscription and the ability to create Azure OpenAI resources and deployments. \n",
    "\n",
    "1. Create a code environment and install the necessary packages\n",
    "\n",
    "```shell\n",
    "conda env create -n raft python=3.11\n",
    "\n",
    "conda activate raft\n",
    "\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "2. Create a GPT-4o deployment\n",
    "3. Create a GPT-4o mini deployment\n",
    "4. Create an Azure OpenAI resource in North Central US or Sweden Central (regions where gpt-4o-mini fine tuning is supported)\n",
    "5. Create a `.env` file based on the [sample.env](./sample.env) file in this repository to store your credentials and important environment variables. Paste your AOAI endpoints, keys and deployment names, name the file `.env`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from typing import Literal, Any\n",
    "import os\n",
    "from math import ceil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "generator_client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AOAI_GPT4o_ENDPOINT\"),\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=os.getenv(\"AOAI_GPT4o_API_KEY\")\n",
    ")\n",
    "\n",
    "gpt4o_deployment = os.getenv(\"AOAI_GPT4o_DEPLOYMENT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and chunking domain-specific documents\n",
    "\n",
    "For Retrieval Augmented Fine Tuning, we need to generate Question-Documents-Answer triplets. The first step is to create document chunks based on our domain-specific documents we want to specialize our model on.\n",
    "\n",
    "For this workshop, we will use the publicly available [BMO Better Banking Guide](./data/better_banking_guide_en.pdf)\n",
    "\n",
    "**Take a minute to browse through the PDF and become more familiar with its content**\n",
    "\n",
    "The guide contains information about various banking accounts offered by BMO as well as how-to guides on e.g how to access Account statements etc.\n",
    "\n",
    "The document is in PDF format and contains a number of tables and charts, we will use GPT-4o to convert the pages content to markdown. Credit to Liam Cavanagh for the inspiration.\n",
    "\n",
    "**a. First we'll need to convert the document pages to images encoded in base64**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_base64_urls(pdf_path):\n",
    "  \"\"\"Converts each page of a PDF to a base64 encoded URL starting with 'data:image/jpeg'.\n",
    "\n",
    "  Parameters:\n",
    "    pdf_path: Path to the PDF file.\n",
    "\n",
    "  Returns:\n",
    "    A list of base64 encoded image URLs, one for each page.\n",
    "  \"\"\"\n",
    "\n",
    "  images = convert_from_path(pdf_path)\n",
    "  base64_urls = []\n",
    "\n",
    "  for image in images:\n",
    "    img_byte_arr = BytesIO()\n",
    "    image.save(img_byte_arr, format=\"JPEG\")\n",
    "    img_byte_arr.seek(0)\n",
    "    base64_encoded = base64.b64encode(img_byte_arr.read()).decode('utf-8')\n",
    "    base64_url = f\"data:image/jpeg;base64,{base64_encoded}\"\n",
    "    base64_urls.append(base64_url)\n",
    "\n",
    "  return base64_urls\n",
    "\n",
    "pdf_path = 'data/better_banking_guide_en.pdf'\n",
    "image_data = pdf_to_base64_urls(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Now we can call GPT-4o to convert our images to markdown**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting images to Markdown:   0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting images to Markdown: 100%|██████████| 26/26 [04:41<00:00, 10.83s/it]\n"
     ]
    }
   ],
   "source": [
    "def gpt_image_to_markdown(image_data, client):\n",
    "    \"\"\"\n",
    "    Converts an image to markdown using GPT-4o.\n",
    "    \n",
    "    Parameters:\n",
    "    image_data: A base64 encoded image.\n",
    "    client: An AzureOpenAI client\n",
    "\n",
    "    Returns:\n",
    "    A list of base64 encoded image URLs, one for each page.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\":\"\"\"You are an AI image assistant capable of extracting text from images\n",
    "         Given an image, you must extract the any visible text on the image and return it in Markdown.\n",
    "         You must keep the original layout and formatting of the text as much as possible in Markdown format.\n",
    "         Pay attention to the text size and use headers, subheaders, bold, italic, tables etc where necessary.\"\"\"},\n",
    "         {\"role\":\"user\", \"content\":[{\n",
    "                \"type\":\"image_url\",\n",
    "                \"image_url\":{\n",
    "                    \"url\":image_data\n",
    "                    }\n",
    "         }]}\n",
    "    ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=gpt4o_deployment,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "markdown_doc = \"\"\n",
    "\n",
    "with tqdm(total=len(image_data), desc=\"Converting images to Markdown\") as pbar:\n",
    "    for img_data in image_data:\n",
    "        result = gpt_image_to_markdown(img_data, generator_client)\n",
    "        markdown_doc += \"\\n\" + result\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Let's chunk the Markdown using Langchain**\n",
    "\n",
    "We first use Langchain's `MarkdownHeaderTextSplitter` to split the document based on headers and then further split the chunks using `RecursiveCharacterTextSplitter` with a chunk size of 1024. Finally, we filter out any chunks that are too short to contain any valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks after markdown split: 15\n",
      "Number of chunks after markdown + recursive split: 115\n",
      "Number of chunks after filtering out empty: 110\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "def remove_special_characters(string):\n",
    "    \"\"\"\n",
    "    Remove special characters from a string.\n",
    "    \n",
    "    Parameters:  \n",
    "    string (str): The input string from which special characters need to be removed.  \n",
    "  \n",
    "    Returns:  \n",
    "    str: A new string with special characters removed.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', string)\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\")    \n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=False\n",
    "    )\n",
    "\n",
    "markdown_doc_splits = markdown_splitter.split_text(markdown_doc)\n",
    "print(f\"Number of chunks after markdown split: {len(markdown_doc_splits)}\")\n",
    "\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "\n",
    "chunked_document = text_splitter.split_documents(markdown_doc_splits)\n",
    "\n",
    "print(f\"Number of chunks after markdown + recursive split: {len(chunked_document)}\")\n",
    "\n",
    "chunks = [chunk.page_content for chunk in chunked_document if len(remove_special_characters(chunk.page_content))>100]\n",
    "\n",
    "print(f\"Number of chunks after filtering out empty: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate training data from the chunked documents\n",
    "\n",
    "We define 2 main functions to generate our Question-Document-Answer triplets from our chunked document\n",
    "\n",
    "1. `generate_instructions_gen()`: This function generates a list of questions based on an input document chunk\n",
    "2. `generate_label()`: This function generates an Answer based on a Question-Document chunk pair\n",
    "\n",
    "**a. First, lets look at the `generate_instructions_gen()` function on a sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_str(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function for helping format strings returned by GPT-4o.\n",
    "    \n",
    "    Parameters:  \n",
    "    s (str): The input string to be formatted.  \n",
    "  \n",
    "    Returns:  \n",
    "    str: A formatted string \n",
    "    \"\"\"\n",
    "    l, r = 0, len(s)-1\n",
    "    beg_found = False\n",
    "    for i in range(len(s)):\n",
    "        if s[i].isalpha():\n",
    "            if not beg_found:\n",
    "                l = i\n",
    "                beg_found = True\n",
    "            else:\n",
    "                r = i \n",
    "    r += 2\n",
    "    return s[l:min(r, len(s))]\n",
    "\n",
    "def generate_instructions_gen(client: AzureOpenAI, chunk: Any, x: int = 5, model: str = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates a list of questions or use cases based on a provided chunk of context using an Azure OpenAI model.  \n",
    "\n",
    "    Parameters:  \n",
    "    client (AzureOpenAI): An instance of the Azure OpenAI client used to communicate with the OpenAI API.  \n",
    "    chunk (Any): The context or chunk of text based on which the questions are to be generated.  \n",
    "    x (int, optional): The number of questions to generate. Default is 5.  \n",
    "    model (str, optional): The specific model to use for generating the questions. Default is None, which uses the default model configured in the client.  \n",
    "  \n",
    "    Returns:  \n",
    "    list[str]: A list of generated questions.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a synthetic question-answer pair generator. Given a chunk of context about some topic(s), generate exactly %s example questions a user could ask and would be answered using information from the chunk. For example, if the given context was a Wikipedia paragraph about the United States, an example question could be 'How many states are in the United States?'\" % (x)},\n",
    "            {\"role\": \"system\", \"content\": \"The questions should be able to be answered in a few words or less. Include only the questions in your response.\"},\n",
    "            {\"role\": \"user\", \"content\": str(chunk)}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    queries = response.choices[0].message.content.split('\\n')\n",
    "    queries = [strip_str(q) for q in queries]\n",
    "    queries = [q for q in queries if any(c.isalpha() for c in q)]\n",
    "    return queries[:int(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize an example picked randomly from our Document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = random.randint(0, len(chunks)-1)\n",
    "chunk = chunks[sample_index]\n",
    "\n",
    "queries = generate_instructions_gen(generator_client, chunk, x=5, model=gpt4o_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ✔️ Multiple Canadian or U.S. dollar personal accounts* (except for Smart Saver Account), including accounts held jointly with, or individually by, your spouse, are covered by one monthly Banking Plan fee for up to 20 accounts (all accounts covered by the Banking Plan are subject to the Banking Plan’s monthly transaction limit where applicable).\n",
      "- ✔️ Your Banking Plan must have a lead account (Canadian or U.S. dollar) from which the monthly Banking Plan fee and transaction fees that exceed the monthly transaction limit will be debited².\n",
      "- ✔️ The monthly Banking Plan fee can be eliminated on specified Plans by maintaining the minimum monthly balance indicated at all times in your Primary Chequing account (when it has been designated as the lead account for your Banking Plan).\n",
      "- ✔️ When your Banking Plan’s monthly transaction limits are exceeded, additional fees will apply on a per-item basis (see Banking Plan fees chart).\n"
     ]
    }
   ],
   "source": [
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How many accounts can be covered by a single monthly Banking Plan fee?',\n",
       " \"What types of accounts are excluded from the Banking Plan's coverage?\",\n",
       " 'What is required for a Banking Plan to have its monthly fee eliminated?',\n",
       " 'Where will the monthly Banking Plan fee be debited from?',\n",
       " \"What happens when the Banking Plan's monthly transaction limits are exceeded?\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Generating questions, answers and adding distractor documents** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "def encode_question_gen(question: str, chunk: Any) -> list[str]:\n",
    "    \"\"\"\n",
    "    Encode multiple prompt instructions into a single string for the general case (`pdf`, `json`, or `txt`).\n",
    "\n",
    "    Parameters:  \n",
    "    question (str): The question to be answered.  \n",
    "    chunk (Any): The context or chunk of text that provides the information needed to answer the question.  \n",
    "  \n",
    "    Returns:  \n",
    "    list[str]: A list of messages formatted for the language model API, including system and user roles.  \n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = []\n",
    "        \n",
    "    prompt = \"\"\"\n",
    "        Question: {question}\\n Context: {context}\\n\n",
    "        Answer this question using the information given in the context above and no prior knowledge. Here is things to pay attention to: \n",
    "        - First provide step-by-step reasoning on how to answer the question. \n",
    "        - In the reasoning, if you need to copy paste some sentences from the context, include them in ##begin_quote## and ##end_quote##. This would mean that things outside of ##begin_quote## and ##end_quote## are not directly copy paste from the context. \n",
    "        - End your response with final answer in the form <ANSWER>: $answer, the answer should be given in a joyful and friendly tone.\n",
    "        - If the answer cannot be found in the context, say \"I'm sorry, I cannot answer this question as I'm missing the required information\"\n",
    "        You MUST begin your final answer with the tag \"<ANSWER>:\".\n",
    "    \"\"\".format(question=question, context=str(chunk))\n",
    "    prompts.append({\"role\": \"system\", \"content\": \"You are a helpful question answerer who can provide an answer given a question and relevant context.\"})\n",
    "    prompts.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return prompts\n",
    "\n",
    "def generate_label(client: AzureOpenAI, question: str, context: Any, model: str = None) -> str | None:\n",
    "    \"\"\"\n",
    "    Generates the label / answer to `question` using `context` and GPT-4o.\n",
    "\n",
    "    Parameters:  \n",
    "    client (AzureOpenAI): An instance of the Azure OpenAI client used to communicate with the OpenAI API.  \n",
    "    question (str): The question to be answered.  \n",
    "    context (Any): The context or chunk of text that provides the information needed to answer the question.  \n",
    "    model (str, optional): The specific model to use for generating the answer. Default is None, which uses the default model configured in the client.  \n",
    "  \n",
    "    Returns:  \n",
    "    str | None: The generated answer from the language model, or None if no answer was generated.\n",
    "    \"\"\"\n",
    "    question = encode_question_gen(question, context)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=question,\n",
    "        n=1,\n",
    "        temperature=0\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "def add_chunk_to_dataset(\n",
    "    client: AzureOpenAI,\n",
    "    chunks: list[str], \n",
    "    chunk: str, \n",
    "    x: int = 5, \n",
    "    num_distract: int = 3, \n",
    "    p: float = 0.8,\n",
    "    model: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Given a chunk, create {Q, A, D} triplets and add them to the dataset.\n",
    "\n",
    "     Parameters:  \n",
    "    client (AzureOpenAI): An instance of the Azure OpenAI client used to communicate with the OpenAI API.  \n",
    "    chunks (list[str]): A list of chunks of text from which distractor documents can be sampled.  \n",
    "    chunk (str): The chunk of text to use as the primary context for generating questions and answers.  \n",
    "    x (int, optional): The number of questions to generate for the given chunk. Default is 5.  \n",
    "    num_distract (int, optional): The number of distractor documents to include with each question. Default is 3.  \n",
    "    p (float, optional): The probability of including the oracle (original) document as part of the context. Default is 0.8.  \n",
    "    model (str, optional): The specific model to use for generating questions and answers. Default is None, which uses the default model configured in the client. \n",
    "    \"\"\"\n",
    "    global ds\n",
    "    global errors\n",
    "    i = chunks.index(chunk)\n",
    "    try:\n",
    "        qs = generate_instructions_gen(client, chunk, x, model)\n",
    "    except Exception as e:\n",
    "        errors.append(e)\n",
    "        return None\n",
    "    for q in qs:\n",
    "        datapt = {\n",
    "            \"id\": None,\n",
    "            \"type\": None,\n",
    "            \"question\": None,\n",
    "            \"context\": None,\n",
    "            \"oracle_context\": None,\n",
    "            \"cot_answer\": None\n",
    "        }\n",
    "\n",
    "        datapt[\"id\"] = f\"seed_task_{i}\"\n",
    "        datapt[\"type\"] = \"general\"\n",
    "        datapt[\"question\"] = q\n",
    "\n",
    "        # add num_distract distractor docs\n",
    "        docs = [chunk]\n",
    "        indices = list(range(0, len(chunks)))\n",
    "        indices.remove(i)\n",
    "        for j in random.sample(indices, num_distract):\n",
    "            docs.append(chunks[j])\n",
    "        \n",
    "        # decides whether to add oracle document\n",
    "        oracle = random.uniform(0, 1) < p\n",
    "        if not oracle:\n",
    "            docs[0] = chunks[random.sample(indices, 1)[0]]\n",
    "        random.shuffle(docs)\n",
    "\n",
    "        d = {\n",
    "            \"title\": [],\n",
    "            \"sentences\": []\n",
    "        }\n",
    "\n",
    "        d[\"title\"].append([\"placeholder_title\"]*(num_distract+1))\n",
    "        d[\"sentences\"].append(docs)\n",
    "        datapt[\"context\"] = d\n",
    "        datapt[\"oracle_context\"] = chunk\n",
    "\n",
    "        # add answer to q\n",
    "        try:\n",
    "            datapt[\"cot_answer\"] = generate_label(client, q, chunk, model=model)\n",
    "        except Exception as e:\n",
    "            errors.append(e)\n",
    "            continue\n",
    "\n",
    "        # construct model instruction \n",
    "        context = \"\"\n",
    "        for doc in docs:\n",
    "            context += \"<DOCUMENT>\" + str(doc) + \"</DOCUMENT>\\n\"\n",
    "        context += q\n",
    "        datapt[\"instruction\"] = context\n",
    "\n",
    "        # add to dataset\n",
    "        if not ds:\n",
    "            # init ds\n",
    "            datapt[\"id\"] = [datapt[\"id\"]]\n",
    "            datapt[\"type\"] = [datapt[\"type\"]]\n",
    "            datapt[\"question\"] = [datapt[\"question\"]]\n",
    "            datapt[\"context\"] = [datapt[\"context\"]]\n",
    "            datapt[\"oracle_context\"] = [datapt[\"oracle_context\"]]\n",
    "            datapt[\"cot_answer\"] = [datapt[\"cot_answer\"]]\n",
    "            datapt[\"instruction\"] = [datapt[\"instruction\"]]\n",
    "            ds = Dataset.from_dict(datapt)\n",
    "        else:\n",
    "            ds = ds.add_item(datapt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's execute this function a in multi-threaded way to speed up the process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 110/110 [02:11<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processing errors: []/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "errors = []\n",
    "ds = Dataset.from_dict({})\n",
    "\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    add_chunk_to_dataset(generator_client, chunks, chunk, 5, 3, model=gpt4o_deployment)\n",
    "\n",
    "# Create a ThreadPoolExecutor with the desired number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the tasks to the executor and store the Future objects\n",
    "    futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "\n",
    "    # Use tqdm to create a progress bar\n",
    "    with tqdm(total=len(chunks), desc=\"Processing chunks\") as pbar:\n",
    "        # Iterate over the completed futures as they become available\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            # Get the result of the completed future\n",
    "            result = future.result()\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "# Print any errors that occurred during processing\n",
    "print(f'Number of processing errors: {errors}/{len(chunks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526 rows and 7 columns in the training dataset\n"
     ]
    }
   ],
   "source": [
    "training_df = ds.to_pandas()\n",
    "\n",
    "print(f'{training_df.shape[0]} rows and {training_df.shape[1]} columns in the training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>oracle_context</th>\n",
       "      <th>cot_answer</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seed_task_5</td>\n",
       "      <td>general</td>\n",
       "      <td>Is the service available in U.S. dollars?</td>\n",
       "      <td>{'sentences': [['BMO has a preferential arrang...</td>\n",
       "      <td>| is available in U.S. dollars                ...</td>\n",
       "      <td>To determine if the service is available in U....</td>\n",
       "      <td>&lt;DOCUMENT&gt;BMO has a preferential arrangement w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seed_task_10</td>\n",
       "      <td>general</td>\n",
       "      <td>How much is the fee for an account history inq...</td>\n",
       "      <td>{'sentences': [['|                            ...</td>\n",
       "      <td>|                                           | ...</td>\n",
       "      <td>To answer the question about the fee for an ac...</td>\n",
       "      <td>&lt;DOCUMENT&gt;|                                   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id     type                                           question  \\\n",
       "0   seed_task_5  general          Is the service available in U.S. dollars?   \n",
       "1  seed_task_10  general  How much is the fee for an account history inq...   \n",
       "\n",
       "                                             context  \\\n",
       "0  {'sentences': [['BMO has a preferential arrang...   \n",
       "1  {'sentences': [['|                            ...   \n",
       "\n",
       "                                      oracle_context  \\\n",
       "0  | is available in U.S. dollars                ...   \n",
       "1  |                                           | ...   \n",
       "\n",
       "                                          cot_answer  \\\n",
       "0  To determine if the service is available in U....   \n",
       "1  To answer the question about the fee for an ac...   \n",
       "\n",
       "                                         instruction  \n",
       "0  <DOCUMENT>BMO has a preferential arrangement w...  \n",
       "1  <DOCUMENT>|                                   ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previewing the generated data\n",
    "\n",
    "training_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Formatting the data in chat format for fine tuning with Azure OpenAI**\n",
    "\n",
    "The conversational chat format is required to fine-tune gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df[\"messages\"] = training_df.apply(lambda x: [\n",
    "                                                     {\"role\":\"user\", \"content\":x['instruction']},\n",
    "                                                     {\"role\":\"assistant\", \"content\":x['cot_answer']}\n",
    "                                                     ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '<DOCUMENT>**Note:**\\nCheques made to yourself or to cash, and cashed at any BMO Bank of Montreal branch, may not be returned but are described on your statement.\\n\\nFor more information on accounts no longer offered, see page 15. ¹¹¹\\n\\n¹ Options may vary by type of personal account.\\n² Based on minimum monthly balance.\\n³ Based on debit card purchases made: refunds from your Primary Chequing account, with one of the following Everyday Banking Plans: Performance, Premium, Senior Plan, with AIR MILES. A maximum of 40 reward miles can be earned per month; Primary chequing must be the lead account. The lead account is the one you designate to pay any fees required by your Banking (for example, monthly plan fees, transaction fees)\\n⁴ If a U.S. dollar account is selected as the Lead Account, all plan and account fees will be charged in U.S. Dollars.\\n\\nBill payments, pre-authorized debits, debit card purchases, Interac e-Transfer, online purchases and cheques and are not permitted on this account.</DOCUMENT>\\n<DOCUMENT>| Minimum charge (per purchase)                    | $3.00                                       |\\n| **Cashing travellers cheques**                   |                                             |\\n| Traveller cheques cashing fee                    | $3.00                                       |\\n| for non-BMO customers                            |                                             |\\n| (excluding American Express Travellers Cheques)  |                                             |\\n| **Courier fees when purchasing travellers        |                                             |\\n| cheques and foreign currency using Online        |                                             |\\n| or Telephone Banking**                           |                                             |\\n| Delivered to a Canadian address                  | $8.50                                       |\\n| **Safety deposit boxes¹**                        |                                             |</DOCUMENT>\\n<DOCUMENT>| The kind of discounted banking I would get | How I would qualify | How I can earn AIR MILES reward miles on purchases made with my debit card |\\n|--------------------------------------------|---------------------|--------------------------------------------------------------------------|\\n| **Kids (0–12 years old)** **/Teens (13–18)**                  | - Free Plus Plan  - Or $9.95 discount towards the Performance or Premium Plan | - Register at any BMO branch by providing proof of age.   | - Use your $9.95 discount to upgrade to the Performance or Premium Plan.   |\\n| **Students (19 or older)**                                   | - Free Plus Plan  - Or $9.95 discount towards the Performance or Premium Plan | - Register at any BMO branch by providing proof of age.  - Annual proof of full-time registration in a post-secondary university or college or registered vocational school due by Nov. 1st of each year you’re enrolled.   | - Use your $9.95 discount to upgrade to the Performance or Premium Plan.   |</DOCUMENT>\\n<DOCUMENT>| non-BMO Bank of Montreal channels²**             | Inside Canada (Interac network)             | $1.50 |\\n|                                                  | Inside U.S.³                                | $3.00 |\\n|                                                  | Outside Canada and U.S. (Cirrus and         | $5.00 |\\n|                                                  | Maestro networks)³                          |       |\\n|                                                  | **Debit card purchase**                     |       |\\n|                                                  | Outside Canada⁴                             | No additional fee |\\n| **Registered Plans**                             |                                             |\\n| A fee of $50 (plus applicable taxes) may be      |                                             |\\n| applied to a registered plan account if you      |                                             |</DOCUMENT>\\nWhere can cheques made to yourself or to cash be cashed?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'To answer the question \"Where can cheques made to yourself or to cash be cashed?\" using the provided context, let\\'s follow a step-by-step reasoning process:\\n\\n1. Identify the relevant part of the context that mentions cheques made to yourself or to cash.\\n2. Look for any specific locations or institutions mentioned where these cheques can be cashed.\\n3. Ensure that the information is directly related to the question.\\n\\nStep 1: The context mentions cheques made to yourself or to cash in the following part:\\n##begin_quote##\\nCheques made to yourself or to cash, and cashed at any BMO Bank of Montreal branch, may not be returned but are described on your statement.\\n##end_quote##\\n\\nStep 2: The relevant part of the context specifies that these cheques can be cashed at any BMO Bank of Montreal branch.\\n\\nStep 3: Confirm that this information directly answers the question.\\n\\n<ANSWER>: You can cash cheques made to yourself or to cash at any BMO Bank of Montreal branch. Have a great day! 😊'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.messages.values[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.dropna(subset=['cot_answer'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spitting our data into training and test sets\n",
    "\n",
    "Splitting your data into training, validation and testing sets when fine-tuning a large language model (LLM) is crucial for ensuring the model's performance and generalization capabilities. The training set is used to teach the model, allowing it to learn patterns from the data. The validation set is used to track performance metrics during the training to avoid underfitting / overfitting. However, to objectively evaluate how well the model has learned and to ensure it can generalize to unseen data, a separate testing set is necessary. \n",
    "\n",
    "We will use this test set in order to measure the improvement of performance we get from using RAFT over RAG with gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 420, Validate: 53, Test: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vhoudebine/miniconda3/envs/openai/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "train_df, validate_df, test_df = np.split(\n",
    "    training_df.sample(frac=1, random_state=42), \n",
    "                       [int(.8*len(training_df)), int(.9*len(training_df))]\n",
    "                       )\n",
    "\n",
    "print(f\"Train: {train_df.shape[0]}, Validate: {validate_df.shape[0]}, Test: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(\"data/training_data\"):\n",
    "    os.makedirs(\"data/training_data\")\n",
    "\n",
    "train_df[['messages']].to_json(\"data/training_data/banking_train.jsonl\", orient=\"records\", lines=True)\n",
    "test_df.to_json(\"data/training_data/banking_test.jsonl\", orient=\"records\", lines=True)\n",
    "validate_df[['messages']].to_json(\"data/training_data/banking_validation.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congrats! We now have a labelled training dataset and a test dataset to evaluate our model's performance. Now go to the [finetuning notebook](./raft_finetuning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
