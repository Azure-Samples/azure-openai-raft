{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RAFT training data generation using GPT-4o\n",
    "\n",
    "In this notebook, we'll synthesize some training data that will eventually be used to fine-tune a GPT-4o mini model in order to adapt it to a set of document(s) and specific domain. **This step is critical as the quality of your training data will greatly influence the performance of your fine-tuned model.**\n",
    "\n",
    "For RAFT, these are the different steps to prepare the training dataset:\n",
    "- Collect Domain-Specific Documents: Gather documents relevant to the domain you want to specialize the LLM in (e.g., medical documents for PubMed, legal documents, API documentation for software).\n",
    "- Chunk the file into Documents\n",
    "- For each Document chunk, generate a set of Questions that can be answered from the Document\n",
    "- For each Document-Question pair, create a list of documents using:\n",
    "    - **Golden Document (D*)**: Document that contains the answer to the question.\n",
    "    - **Distractor Documents (Dk)**: Documents that do not contain relevant information.\n",
    "- Question-Answer-Document Triplets: From each **Document-Question** pair, generate a factual **Answer** based on the Golden Document.\n",
    "\n",
    "Curating a good training dataset often involves manual work and review by SMEs. That said, we can use an LLM to help us generate an initial set of training examples that can be vetted and further refined by SMEs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Pre-requisites\n",
    "\n",
    "For this hands-on workshop, all you need is access to an Azure subscription and the ability to create Azure OpenAI resources and deployments. \n",
    "\n",
    "0. Install poppler for PDF processing\n",
    "\n",
    "- on Linux run `sudo apt-get install -y poppler-utils`\n",
    "- on Mac run `brew install poppler`\n",
    "- on Windows run `conda install -c conda-forge poppler`\n",
    "\n",
    "1. Create a code environment and install the necessary packages\n",
    "\n",
    "```shell\n",
    "conda create -n raft python=3.11\n",
    "\n",
    "conda activate raft\n",
    "\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "2. Create a GPT-4o deployment\n",
    "3. Create a GPT-4o mini deployment\n",
    "4. Create an Azure OpenAI resource in North Central US or Sweden Central (regions where gpt-4o-mini fine tuning is supported)\n",
    "5. Create a `.env` file based on the [sample.env](./sample.env) file in this repository to store your credentials and important environment variables. Paste your AOAI endpoints, keys and deployment names, name the file `.env`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpdf2image\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_from_path\n\u001b[1;32m     13\u001b[0m load_dotenv()\n\u001b[0;32m---> 15\u001b[0m generator_client \u001b[38;5;241m=\u001b[39m \u001b[43mAzureOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mazure_endpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAOAI_GPT4o_ENDPOINT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2024-02-01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAOAI_GPT4o_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m gpt4o_deployment \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAOAI_GPT4o_DEPLOYMENT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/lib/azure.py:205\u001b[0m, in \u001b[0;36mAzureOpenAI.__init__\u001b[0;34m(self, api_version, azure_endpoint, azure_deployment, api_key, azure_ad_token, azure_ad_token_provider, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# define a sentinel value to avoid any typing issues\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m API_KEY_SENTINEL\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_version \u001b[38;5;241m=\u001b[39m api_version\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_azure_ad_token \u001b[38;5;241m=\u001b[39m azure_ad_token\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_client.py:123\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.openai.com/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_stream_cls \u001b[38;5;241m=\u001b[39m Stream\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletions \u001b[38;5;241m=\u001b[39m resources\u001b[38;5;241m.\u001b[39mCompletions(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_base_client.py:843\u001b[0m, in \u001b[0;36mSyncAPIClient.__init__\u001b[0;34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    827\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    828\u001b[0m     )\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    831\u001b[0m     version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    832\u001b[0m     limits\u001b[38;5;241m=\u001b[39mlimits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    841\u001b[0m     _strict_response_validation\u001b[38;5;241m=\u001b[39m_strict_response_validation,\n\u001b[1;32m    842\u001b[0m )\n\u001b[0;32m--> 843\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_base_client.py:741\u001b[0m, in \u001b[0;36m_DefaultHttpxClient.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimits\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[1;32m    740\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 741\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Client.__init__() got an unexpected keyword argument 'proxies'"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from typing import Literal, Any\n",
    "import os\n",
    "from math import ceil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "generator_client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AOAI_GPT4o_ENDPOINT\"),\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=os.getenv(\"AOAI_GPT4o_API_KEY\")\n",
    ")\n",
    "\n",
    "gpt4o_deployment = os.getenv(\"AOAI_GPT4o_DEPLOYMENT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and chunking domain-specific documents\n",
    "\n",
    "For Retrieval Augmented Fine Tuning, we need to generate Question-Documents-Answer triplets. The first step is to create document chunks based on our domain-specific documents we want to specialize our model on.\n",
    "\n",
    "For this workshop, we will use the publicly available [BMO Better Banking Guide](./data/better_banking_guide_en.pdf)\n",
    "\n",
    "**Take a minute to browse through the PDF and become more familiar with its content**\n",
    "\n",
    "The guide contains information about various banking accounts offered by BMO as well as how-to guides on e.g how to access Account statements etc.\n",
    "\n",
    "The document is in PDF format and contains a number of tables and charts, we will use GPT-4o to convert the pages content to markdown. Credit to Liam Cavanagh for the inspiration.\n",
    "\n",
    "**a. First we'll need to convert the document pages to images encoded in base64**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "PDFInfoNotInstalledError",
     "evalue": "Unable to get page count. Is poppler installed and in PATH?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pdf2image/pdf2image.py:581\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[1;32m    580\u001b[0m     env[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m poppler_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m env\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 581\u001b[0m proc \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/subprocess.py:1950\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pdfinfo'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPDFInfoNotInstalledError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m base64_urls\n\u001b[1;32m     24\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/better_banking_guide_en.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 25\u001b[0m image_data \u001b[38;5;241m=\u001b[39m \u001b[43mpdf_to_base64_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mpdf_to_base64_urls\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpdf_to_base64_urls\u001b[39m(pdf_path):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts each page of a PDF to a base64 encoded URL starting with 'data:image/jpeg'.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m  Parameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    A list of base64 encoded image URLs, one for each page.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m   images \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m   base64_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pdf2image/pdf2image.py:127\u001b[0m, in \u001b[0;36mconvert_from_path\u001b[0;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(poppler_path, PurePath):\n\u001b[1;32m    125\u001b[0m     poppler_path \u001b[38;5;241m=\u001b[39m poppler_path\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[0;32m--> 127\u001b[0m page_count \u001b[38;5;241m=\u001b[39m \u001b[43mpdfinfo_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mownerpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoppler_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoppler_path\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# We start by getting the output format, the buffer processing function and if we need pdftocairo\u001b[39;00m\n\u001b[1;32m    132\u001b[0m parsed_fmt, final_extension, parse_buffer_func, use_pdfcairo_format \u001b[38;5;241m=\u001b[39m _parse_format(\n\u001b[1;32m    133\u001b[0m     fmt, grayscale\n\u001b[1;32m    134\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pdf2image/pdf2image.py:607\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFInfoNotInstalledError(\n\u001b[1;32m    608\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get page count. Is poppler installed and in PATH?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFPageCountError(\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get page count.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m     )\n",
      "\u001b[0;31mPDFInfoNotInstalledError\u001b[0m: Unable to get page count. Is poppler installed and in PATH?"
     ]
    }
   ],
   "source": [
    "def pdf_to_base64_urls(pdf_path):\n",
    "  \"\"\"Converts each page of a PDF to a base64 encoded URL starting with 'data:image/jpeg'.\n",
    "\n",
    "  Parameters:\n",
    "    pdf_path: Path to the PDF file.\n",
    "\n",
    "  Returns:\n",
    "    A list of base64 encoded image URLs, one for each page.\n",
    "  \"\"\"\n",
    "\n",
    "  images = convert_from_path(pdf_path)\n",
    "  base64_urls = []\n",
    "\n",
    "  for image in images:\n",
    "    img_byte_arr = BytesIO()\n",
    "    image.save(img_byte_arr, format=\"JPEG\")\n",
    "    img_byte_arr.seek(0)\n",
    "    base64_encoded = base64.b64encode(img_byte_arr.read()).decode('utf-8')\n",
    "    base64_url = f\"data:image/jpeg;base64,{base64_encoded}\"\n",
    "    base64_urls.append(base64_url)\n",
    "\n",
    "  return base64_urls\n",
    "\n",
    "pdf_path = './data/better_banking_guide_en.pdf'\n",
    "image_data = pdf_to_base64_urls(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Now we can call GPT-4o to convert our images to markdown**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     37\u001b[0m markdown_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mimage_data\u001b[49m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting images to Markdown\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img_data \u001b[38;5;129;01min\u001b[39;00m image_data:\n\u001b[1;32m     41\u001b[0m         result \u001b[38;5;241m=\u001b[39m gpt_image_to_markdown(img_data, generator_client)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_data' is not defined"
     ]
    }
   ],
   "source": [
    "def gpt_image_to_markdown(image_data, client):\n",
    "    \"\"\"\n",
    "    Converts an image to markdown using GPT-4o.\n",
    "    \n",
    "    Parameters:\n",
    "    image_data: A base64 encoded image.\n",
    "    client: An AzureOpenAI client\n",
    "\n",
    "    Returns:\n",
    "    A list of base64 encoded image URLs, one for each page.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\":\"\"\"You are an AI image assistant capable of extracting text from images\n",
    "         Given an image, you must extract the any visible text on the image and return it in Markdown.\n",
    "         You must keep the original layout and formatting of the text as much as possible in Markdown format.\n",
    "         Pay attention to the text size and use headers, subheaders, bold, italic, tables etc where necessary.\"\"\"},\n",
    "         {\"role\":\"user\", \"content\":[{\n",
    "                \"type\":\"image_url\",\n",
    "                \"image_url\":{\n",
    "                    \"url\":image_data\n",
    "                    }\n",
    "         }]}\n",
    "    ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=gpt4o_deployment,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "markdown_doc = \"\"\n",
    "\n",
    "with tqdm(total=len(image_data), desc=\"Converting images to Markdown\") as pbar:\n",
    "    for img_data in image_data:\n",
    "        result = gpt_image_to_markdown(img_data, generator_client)\n",
    "        markdown_doc += \"\\n\" + result\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Let's chunk the Markdown using Langchain**\n",
    "\n",
    "We first use Langchain's `MarkdownHeaderTextSplitter` to split the document based on headers and then further split the chunks using `RecursiveCharacterTextSplitter` with a chunk size of 1024. Finally, we filter out any chunks that are too short to contain any valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "def remove_special_characters(string):\n",
    "    \"\"\"\n",
    "    Remove special characters from a string.\n",
    "    \n",
    "    Parameters:  \n",
    "    string (str): The input string from which special characters need to be removed.  \n",
    "  \n",
    "    Returns:  \n",
    "    str: A new string with special characters removed.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', string)\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\")    \n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=False\n",
    "    )\n",
    "\n",
    "markdown_doc_splits = markdown_splitter.split_text(markdown_doc)\n",
    "print(f\"Number of chunks after markdown split: {len(markdown_doc_splits)}\")\n",
    "\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "\n",
    "chunked_document = text_splitter.split_documents(markdown_doc_splits)\n",
    "\n",
    "print(f\"Number of chunks after markdown + recursive split: {len(chunked_document)}\")\n",
    "\n",
    "chunks = [chunk.page_content for chunk in chunked_document if len(remove_special_characters(chunk.page_content))>100]\n",
    "\n",
    "print(f\"Number of chunks after filtering out empty: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate training data from the chunked documents\n",
    "\n",
    "We define 2 main functions to generate our Question-Document-Answer triplets from our chunked document\n",
    "\n",
    "1. `generate_instructions_gen()`: This function generates a list of questions based on an input document chunk\n",
    "2. `generate_label()`: This function generates an Answer based on a Question-Document chunk pair\n",
    "\n",
    "**a. First, lets look at the `generate_instructions_gen()` function on a sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_str(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function for helping format strings returned by GPT-4o.\n",
    "    \n",
    "    Parameters:  \n",
    "    s (str): The input string to be formatted.  \n",
    "  \n",
    "    Returns:  \n",
    "    str: A formatted string \n",
    "    \"\"\"\n",
    "    l, r = 0, len(s)-1\n",
    "    beg_found = False\n",
    "    for i in range(len(s)):\n",
    "        if s[i].isalpha():\n",
    "            if not beg_found:\n",
    "                l = i\n",
    "                beg_found = True\n",
    "            else:\n",
    "                r = i \n",
    "    r += 2\n",
    "    return s[l:min(r, len(s))]\n",
    "\n",
    "def generate_instructions_gen(client: AzureOpenAI, chunk: Any, x: int = 5, model: str = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates a list of questions or use cases based on a provided chunk of context using an Azure OpenAI model.  \n",
    "\n",
    "    Parameters:  \n",
    "    client (AzureOpenAI): An instance of the Azure OpenAI client used to communicate with the OpenAI API.  \n",
    "    chunk (Any): The context or chunk of text based on which the questions are to be generated.  \n",
    "    x (int, optional): The number of questions to generate. Default is 5.  \n",
    "    model (str, optional): The specific model to use for generating the questions. Default is None, which uses the default model configured in the client.  \n",
    "  \n",
    "    Returns:  \n",
    "    list[str]: A list of generated questions.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a synthetic question-answer pair generator. Given a chunk of context about some topic(s), generate exactly %s example questions a user could ask and would be answered using information from the chunk. For example, if the given context was a Wikipedia paragraph about the United States, an example question could be 'How many states are in the United States?'\" % (x)},\n",
    "            {\"role\": \"system\", \"content\": \"The questions should be able to be answered in a few words or less. Include only the questions in your response.\"},\n",
    "            {\"role\": \"user\", \"content\": str(chunk)}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    queries = response.choices[0].message.content.split('\\n')\n",
    "    queries = [strip_str(q) for q in queries]\n",
    "    queries = [q for q in queries if any(c.isalpha() for c in q)]\n",
    "    return queries[:int(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize an example picked randomly from our Document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = random.randint(0, len(chunks)-1)\n",
    "chunk = chunks[sample_index]\n",
    "\n",
    "queries = generate_instructions_gen(generator_client, chunk, x=5, model=gpt4o_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Generating questions, answers and adding distractor documents** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "def encode_question_gen(question: str, chunk: Any) -> list[str]:\n",
    "    \"\"\"\n",
    "    Encode multiple prompt instructions into a single string for the general case (`pdf`, `json`, or `txt`).\n",
    "\n",
    "    Parameters:  \n",
    "    question (str): The question to be answered.  \n",
    "    chunk (Any): The context or chunk of text that provides the information needed to answer the question.  \n",
    "  \n",
    "    Returns:  \n",
    "    list[str]: A list of messages formatted for the language model API, including system and user roles.  \n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = []\n",
    "        \n",
    "    prompt = \"\"\"\n",
    "        Question: {question}\\n Context: {context}\\n\n",
    "        Answer this question using the information given in the context above and no prior knowledge. Here is things to pay attention to: \n",
    "        - First provide step-by-step reasoning on how to answer the question. \n",
    "        - In the reasoning, if you need to copy paste some sentences from the context, include them in ##begin_quote## and ##end_quote##. This would mean that things outside of ##begin_quote## and ##end_quote## are not directly copy paste from the context. \n",
    "        - End your response with final answer in the form <ANSWER>: $answer, the answer should be given in a joyful and friendly tone.\n",
    "        - If the answer cannot be found in the context, say \"I'm sorry, I cannot answer this question as I'm missing the required information\"\n",
    "        You MUST begin your final answer with the tag \"<ANSWER>:\".\n",
    "    \"\"\".format(question=question, context=str(chunk))\n",
    "    prompts.append({\"role\": \"system\", \"content\": \"You are a helpful question answerer who can provide an answer given a question and relevant context.\"})\n",
    "    prompts.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return prompts\n",
    "\n",
    "def generate_label(client: AzureOpenAI, question: str, context: Any, model: str = None) -> str | None:\n",
    "    \"\"\"\n",
    "    Generates the label / answer to `question` using `context` and GPT-4o.\n",
    "\n",
    "    Parameters:  \n",
    "    client (AzureOpenAI): An instance of the Azure OpenAI client used to communicate with the OpenAI API.  \n",
    "    question (str): The question to be answered.  \n",
    "    context (Any): The context or chunk of text that provides the information needed to answer the question.  \n",
    "    model (str, optional): The specific model to use for generating the answer. Default is None, which uses the default model configured in the client.  \n",
    "  \n",
    "    Returns:  \n",
    "    str | None: The generated answer from the language model, or None if no answer was generated.\n",
    "    \"\"\"\n",
    "    question = encode_question_gen(question, context)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=question,\n",
    "        n=1,\n",
    "        temperature=0\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "def add_chunk_to_dataset(\n",
    "    client: AzureOpenAI,\n",
    "    chunks: list[str], \n",
    "    chunk: str, \n",
    "    x: int = 5, \n",
    "    num_distract: int = 3, \n",
    "    p: float = 0.8,\n",
    "    model: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Given a chunk, create {Q, A, D} triplets and add them to the dataset.\n",
    "\n",
    "     Parameters:  \n",
    "    client (AzureOpenAI): An instance of the Azure OpenAI client used to communicate with the OpenAI API.  \n",
    "    chunks (list[str]): A list of chunks of text from which distractor documents can be sampled.  \n",
    "    chunk (str): The chunk of text to use as the primary context for generating questions and answers.  \n",
    "    x (int, optional): The number of questions to generate for the given chunk. Default is 5.  \n",
    "    num_distract (int, optional): The number of distractor documents to include with each question. Default is 3.  \n",
    "    p (float, optional): The probability of including the oracle (original) document as part of the context. Default is 0.8.  \n",
    "    model (str, optional): The specific model to use for generating questions and answers. Default is None, which uses the default model configured in the client. \n",
    "    \"\"\"\n",
    "    global ds\n",
    "    global errors\n",
    "    i = chunks.index(chunk)\n",
    "    try:\n",
    "        qs = generate_instructions_gen(client, chunk, x, model)\n",
    "    except Exception as e:\n",
    "        errors.append(e)\n",
    "        return None\n",
    "    for q in qs:\n",
    "        datapt = {\n",
    "            \"id\": None,\n",
    "            \"type\": None,\n",
    "            \"question\": None,\n",
    "            \"context\": None,\n",
    "            \"oracle_context\": None,\n",
    "            \"cot_answer\": None\n",
    "        }\n",
    "\n",
    "        datapt[\"id\"] = f\"seed_task_{i}\"\n",
    "        datapt[\"type\"] = \"general\"\n",
    "        datapt[\"question\"] = q\n",
    "\n",
    "        # add num_distract distractor docs\n",
    "        docs = [chunk]\n",
    "        indices = list(range(0, len(chunks)))\n",
    "        indices.remove(i)\n",
    "        for j in random.sample(indices, num_distract):\n",
    "            docs.append(chunks[j])\n",
    "        \n",
    "        # decides whether to add oracle document\n",
    "        oracle = random.uniform(0, 1) < p\n",
    "        if not oracle:\n",
    "            docs[0] = chunks[random.sample(indices, 1)[0]]\n",
    "        random.shuffle(docs)\n",
    "\n",
    "        d = {\n",
    "            \"title\": [],\n",
    "            \"sentences\": []\n",
    "        }\n",
    "\n",
    "        d[\"title\"].append([\"placeholder_title\"]*(num_distract+1))\n",
    "        d[\"sentences\"].append(docs)\n",
    "        datapt[\"context\"] = d\n",
    "        datapt[\"oracle_context\"] = chunk\n",
    "\n",
    "        # add answer to q\n",
    "        try:\n",
    "            datapt[\"cot_answer\"] = generate_label(client, q, chunk, model=model)\n",
    "        except Exception as e:\n",
    "            errors.append(e)\n",
    "            continue\n",
    "\n",
    "        # construct model instruction \n",
    "        context = \"\"\n",
    "        for doc in docs:\n",
    "            context += \"<DOCUMENT>\" + str(doc) + \"</DOCUMENT>\\n\"\n",
    "        context += q\n",
    "        datapt[\"instruction\"] = context\n",
    "\n",
    "        # add to dataset\n",
    "        if not ds:\n",
    "            # init ds\n",
    "            datapt[\"id\"] = [datapt[\"id\"]]\n",
    "            datapt[\"type\"] = [datapt[\"type\"]]\n",
    "            datapt[\"question\"] = [datapt[\"question\"]]\n",
    "            datapt[\"context\"] = [datapt[\"context\"]]\n",
    "            datapt[\"oracle_context\"] = [datapt[\"oracle_context\"]]\n",
    "            datapt[\"cot_answer\"] = [datapt[\"cot_answer\"]]\n",
    "            datapt[\"instruction\"] = [datapt[\"instruction\"]]\n",
    "            ds = Dataset.from_dict(datapt)\n",
    "        else:\n",
    "            ds = ds.add_item(datapt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's execute this function a in multi-threaded way to speed up the process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "errors = []\n",
    "ds = Dataset.from_dict({})\n",
    "\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    add_chunk_to_dataset(generator_client, chunks, chunk, 5, 3, model=gpt4o_deployment)\n",
    "\n",
    "# Create a ThreadPoolExecutor with the desired number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the tasks to the executor and store the Future objects\n",
    "    futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "\n",
    "    # Use tqdm to create a progress bar\n",
    "    with tqdm(total=len(chunks), desc=\"Processing chunks\") as pbar:\n",
    "        # Iterate over the completed futures as they become available\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            # Get the result of the completed future\n",
    "            result = future.result()\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "# Print any errors that occurred during processing\n",
    "print(f'Number of processing errors: {errors}/{len(chunks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = ds.to_pandas()\n",
    "\n",
    "print(f'{training_df.shape[0]} rows and {training_df.shape[1]} columns in the training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the generated data\n",
    "\n",
    "training_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Formatting the data in chat format for fine tuning with Azure OpenAI**\n",
    "\n",
    "The conversational chat format is required to fine-tune gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df[\"messages\"] = training_df.apply(lambda x: [\n",
    "                                                     {\"role\":\"user\", \"content\":x['instruction']},\n",
    "                                                     {\"role\":\"assistant\", \"content\":x['cot_answer']}\n",
    "                                                     ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.messages.values[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.dropna(subset=['cot_answer'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spitting our data into training and test sets\n",
    "\n",
    "Splitting your data into training, validation and testing sets when fine-tuning a large language model (LLM) is crucial for ensuring the model's performance and generalization capabilities. The training set is used to teach the model, allowing it to learn patterns from the data. The validation set is used to track performance metrics during the training to avoid underfitting / overfitting. However, to objectively evaluate how well the model has learned and to ensure it can generalize to unseen data, a separate testing set is necessary. \n",
    "\n",
    "We will use this test set in order to measure the improvement of performance we get from using RAFT over RAG with gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "train_df, validate_df, test_df = np.split(\n",
    "    training_df.sample(frac=1, random_state=42), \n",
    "                       [int(.8*len(training_df)), int(.9*len(training_df))]\n",
    "                       )\n",
    "\n",
    "print(f\"Train: {train_df.shape[0]}, Validate: {validate_df.shape[0]}, Test: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(\"./data/training_data\"):\n",
    "    os.makedirs(\"./data/training_data\")\n",
    "\n",
    "train_df[['messages']].to_json(\"./data/training_data/banking_train.jsonl\", orient=\"records\", lines=True)\n",
    "test_df.to_json(\"./data/training_data/banking_test.jsonl\", orient=\"records\", lines=True)\n",
    "validate_df[['messages']].to_json(\"./data/training_data/banking_validation.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congrats! We now have a labelled training dataset and a test dataset to evaluate our model's performance. Now go to the [finetuning notebook](./raft_finetuning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
