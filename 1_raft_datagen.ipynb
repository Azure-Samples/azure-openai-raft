{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RAFT training data generation using GPT-4o\n",
    "\n",
    "In this notebook, we'll synthesize some training data that will eventually be used to fine-tune a GPT-4o mini model in order to adapt it to a set of document(s) and specific domain. **This step is critical as the quality of your training data will greatly influence the performance of your fine-tuned model.**\n",
    "\n",
    "For RAFT, these are the different steps to prepare the training dataset:\n",
    "- Collect Domain-Specific Documents: Gather documents relevant to the domain you want to specialize the LLM in (e.g., medical documents for PubMed, legal documents, API documentation for software).\n",
    "- Chunk the file into Documents\n",
    "- For each Document chunk, generate a set of Questions that can be answered from the Document\n",
    "- For each Document-Question pair, create a list of documents using:\n",
    "    - **Golden Document (D*)**: Document that contains the answer to the question.\n",
    "    - **Distractor Documents (Dk)**: Documents that do not contain relevant information.\n",
    "- Question-Answer-Document Triplets: From each **Document-Question** pair, generate a factual **Answer** based on the Golden Document.\n",
    "\n",
    "Curating a good training dataset often involves manual work and review by SMEs. That said, we can use an LLM to help us generate an initial set of training examples that can be vetted and further refined by SMEs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Pre-requisites\n",
    "\n",
    "For this hands-on workshop, all you need is access to an Azure subscription and the ability to create Azure OpenAI resources and deployments. \n",
    "\n",
    "0. Install poppler for PDF processing\n",
    "\n",
    "- on Linux run `sudo apt-get install -y poppler-utils`\n",
    "- on Mac run `brew install poppler`\n",
    "- on Windows run `conda install -c conda-forge poppler`\n",
    "\n",
    "1. Create a code environment and install the necessary packages\n",
    "\n",
    "```shell\n",
    "conda create -n raft python=3.11\n",
    "\n",
    "conda activate raft\n",
    "\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "2. Create a GPT-4o deployment\n",
    "3. Create a GPT-4o mini deployment\n",
    "4. Create an Azure OpenAI resource in North Central US or Sweden Central (regions where gpt-4o-mini fine tuning is supported)\n",
    "5. Create a `.env` file based on the [sample.env](./sample.env) file in this repository to store your credentials and important environment variables. Paste your AOAI endpoints, keys and deployment names, name the file `.env`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from typing import Literal, Any\n",
    "import os\n",
    "from math import ceil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "generator_client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AOAI_GPT4o_ENDPOINT\"),\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=os.getenv(\"AOAI_GPT4o_API_KEY\")\n",
    ")\n",
    "\n",
    "gpt4o_deployment = os.getenv(\"AOAI_GPT4o_DEPLOYMENT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and chunking domain-specific documents\n",
    "\n",
    "For Retrieval Augmented Fine Tuning, we need to generate Question-Documents-Answer triplets. The first step is to create document chunks based on our domain-specific documents we want to specialize our model on.\n",
    "\n",
    "For this workshop, we will use the publicly available [BMO Better Banking Guide](./data/better_banking_guide_en.pdf)\n",
    "\n",
    "**Take a minute to browse through the PDF and become more familiar with its content**\n",
    "\n",
    "The guide contains information about various banking accounts offered by BMO as well as how-to guides on e.g how to access Account statements etc.\n",
    "\n",
    "The document is in PDF format and contains a number of tables and charts, we will use GPT-4o to convert the pages content to markdown. Credit to Liam Cavanagh for the inspiration.\n",
    "\n",
    "**a. First we'll need to convert the document pages to images encoded in base64**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_base64_urls(pdf_path):\n",
    "  \"\"\"Converts each page of a PDF to a base64 encoded URL starting with 'data:image/jpeg'.\n",
    "\n",
    "  Parameters:\n",
    "    pdf_path: Path to the PDF file.\n",
    "\n",
    "  Returns:\n",
    "    A list of base64 encoded image URLs, one for each page.\n",
    "  \"\"\"\n",
    "\n",
    "  images = convert_from_path(pdf_path)\n",
    "  base64_urls = []\n",
    "\n",
    "  for image in images:\n",
    "    img_byte_arr = BytesIO()\n",
    "    image.save(img_byte_arr, format=\"JPEG\")\n",
    "    img_byte_arr.seek(0)\n",
    "    base64_encoded = base64.b64encode(img_byte_arr.read()).decode('utf-8')\n",
    "    base64_url = f\"data:image/jpeg;base64,{base64_encoded}\"\n",
    "    base64_urls.append(base64_url)\n",
    "\n",
    "  return base64_urls\n",
    "\n",
    "pdf_path = './data/better_banking_guide_en.pdf'\n",
    "image_data = pdf_to_base64_urls(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Now we can call GPT-4o to convert our images to markdown**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_image_to_markdown(image_data, client):\n",
    "    \"\"\"\n",
    "    Converts an image to markdown using GPT-4o.\n",
    "    \n",
    "    Parameters:\n",
    "    image_data: A base64 encoded image.\n",
    "    client: An AzureOpenAI client\n",
    "\n",
    "    Returns:\n",
    "    A list of base64 encoded image URLs, one for each page.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\":\"\"\"You are an AI image assistant capable of extracting text from images\n",
    "         Given an image, you must extract the any visible text on the image and return it in Markdown.\n",
    "         You must keep the original layout and formatting of the text as much as possible in Markdown format.\n",
    "         Pay attention to the text size and use headers, subheaders, bold, italic, tables etc where necessary.\"\"\"},\n",
    "         {\"role\":\"user\", \"content\":[{\n",
    "                \"type\":\"image_url\",\n",
    "                \"image_url\":{\n",
    "                    \"url\":image_data\n",
    "                    }\n",
    "         }]}\n",
    "    ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=gpt4o_deployment,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "markdown_doc = \"\"\n",
    "\n",
    "with tqdm(total=len(image_data), desc=\"Converting images to Markdown\") as pbar:\n",
    "    for img_data in image_data:\n",
    "        result = gpt_image_to_markdown(img_data, generator_client)\n",
    "        markdown_doc += \"\\n\" + result\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Let's chunk the Markdown using Langchain**\n",
    "\n",
    "We first use Langchain's `MarkdownHeaderTextSplitter` to split the document based on headers and then further split the chunks using `RecursiveCharacterTextSplitter` with a chunk size of 1024. Finally, we filter out any chunks that are too short to contain any valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "def remove_special_characters(string):\n",
    "    \"\"\"\n",
    "    Remove special characters from a string.\n",
    "    \n",
    "    Parameters:  \n",
    "    string (str): The input string from which special characters need to be removed.  \n",
    "  \n",
    "    Returns:  \n",
    "    str: A new string with special characters removed.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', string)\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\")    \n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=False\n",
    "    )\n",
    "\n",
    "markdown_doc_splits = markdown_splitter.split_text(markdown_doc)\n",
    "print(f\"Number of chunks after markdown split: {len(markdown_doc_splits)}\")\n",
    "\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "\n",
    "chunked_document = text_splitter.split_documents(markdown_doc_splits)\n",
    "\n",
    "print(f\"Number of chunks after markdown + recursive split: {len(chunked_document)}\")\n",
    "\n",
    "chunks = [chunk.page_content for chunk in chunked_document if len(remove_special_characters(chunk.page_content))>100]\n",
    "\n",
    "print(f\"Number of chunks after filtering out empty: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate training data from the chunked documents\n",
    "\n",
    "We define 2 main functions to generate our Question-Document-Answer triplets from our chunked document\n",
    "\n",
    "1. `generate_instructions_gen()`: This function generates a list of questions based on an input document chunk\n",
    "2. `generate_label()`: This function generates an Answer based on a Question-Document chunk pair\n",
    "\n",
    "**a. First, lets look at the `generate_instructions_gen()` function on a sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_str(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function for helping format strings returned by GPT-4o.\n",
    "    \n",
    "    Parameters:  \n",
    "    s (str): The input string to be formatted.  \n",
    "  \n",
    "    Returns:  \n",
    "    str: A formatted string \n",
    "    \"\"\"\n",
    "    l, r = 0, len(s)-1\n",
    "    beg_found = False\n",
    "    for i in range(len(s)):\n",
    "        if s[i].isalpha():\n",
    "            if not beg_found:\n",
    "                l = i\n",
    "                beg_found = True\n",
    "            else:\n",
    "                r = i \n",
    "    r += 2\n",
    "    return s[l:min(r, len(s))]\n",
    "\n",
    "def generate_instructions_gen(client: AzureOpenAI, chunk: Any, x: int = 5, model: str = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates a list of questions or use cases based on a provided chunk of context using an Azure OpenAI model.  \n",
    "\n",
    "    Parameters:  \n",
    "    client (AzureOpenAI): An instance of the Azure OpenAI client used to communicate with the OpenAI API.  \n",
    "    chunk (Any): The context or chunk of text based on which the questions are to be generated.  \n",
    "    x (int, optional): The number of questions to generate. Default is 5.  \n",
    "    model (str, optional): The specific model to use for generating the questions. Default is None, which uses the default model configured in the client.  \n",
    "  \n",
    "    Returns:  \n",
    "    list[str]: A list of generated questions.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a synthetic question-answer pair generator. Given a chunk of context about some topic(s), generate exactly %s example questions a user could ask and would be answered using information from the chunk. For example, if the given context was a Wikipedia paragraph about the United States, an example question could be 'How many states are in the United States?'\" % (x)},\n",
    "            {\"role\": \"system\", \"content\": \"The questions should be able to be answered in a few words or less. Include only the questions in your response.\"},\n",
    "            {\"role\": \"user\", \"content\": str(chunk)}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    queries = response.choices[0].message.content.split('\\n')\n",
    "    queries = [strip_str(q) for q in queries]\n",
    "    queries = [q for q in queries if any(c.isalpha() for c in q)]\n",
    "    return queries[:int(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize an example picked randomly from our Document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = random.randint(0, len(chunks)-1)\n",
    "chunk = chunks[sample_index]\n",
    "\n",
    "queries = generate_instructions_gen(generator_client, chunk, x=5, model=gpt4o_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Generating questions, answers and adding distractor documents** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "def encode_question_gen(question: str, chunk: Any) -> list[str]:\n",
    "    \"\"\"\n",
    "    Encode multiple prompt instructions into a single string for the general case (`pdf`, `json`, or `txt`).\n",
    "\n",
    "    Parameters:  \n",
    "    question (str): The question to be answered.  \n",
    "    chunk (Any): The context or chunk of text that provides the information needed to answer the question.  \n",
    "  \n",
    "    Returns:  \n",
    "    list[str]: A list of messages formatted for the language model API, including system and user roles.  \n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = []\n",
    "        \n",
    "    prompt = \"\"\"\n",
    "        Question: {question}\\n Context: {context}\\n\n",
    "        Answer this question using the information given in the context above and no prior knowledge. Here is things to pay attention to: \n",
    "        - First provide step-by-step reasoning on how to answer the question. \n",
    "        - In the reasoning, if you need to copy paste some sentences from the context, include them in ##begin_quote## and ##end_quote##. This would mean that things outside of ##begin_quote## and ##end_quote## are not directly copy paste from the context. \n",
    "        - End your response with final answer in the form <ANSWER>: $answer, the answer should be given in a joyful and friendly tone.\n",
    "        - If the answer cannot be found in the context, say \"I'm sorry, I cannot answer this question as I'm missing the required information\"\n",
    "        You MUST begin your final answer with the tag \"<ANSWER>:\".\n",
    "    \"\"\".format(question=question, context=str(chunk))\n",
    "    prompts.append({\"role\": \"system\", \"content\": \"You are a helpful question answerer who can provide an answer given a question and relevant context.\"})\n",
    "    prompts.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return prompts\n",
    "\n",
    "def generate_label(client: AzureOpenAI, question: str, context: Any, model: str = None) -> str | None:\n",
    "    \"\"\"\n",
    "    Generates the label / answer to `question` using `context` and GPT-4o.\n",
    "\n",
    "    Parameters:  \n",
    "    client (AzureOpenAI): An instance of the Azure OpenAI client used to communicate with the OpenAI API.  \n",
    "    question (str): The question to be answered.  \n",
    "    context (Any): The context or chunk of text that provides the information needed to answer the question.  \n",
    "    model (str, optional): The specific model to use for generating the answer. Default is None, which uses the default model configured in the client.  \n",
    "  \n",
    "    Returns:  \n",
    "    str | None: The generated answer from the language model, or None if no answer was generated.\n",
    "    \"\"\"\n",
    "    question = encode_question_gen(question, context)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=question,\n",
    "        n=1,\n",
    "        temperature=0\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "def add_chunk_to_dataset(\n",
    "    client: AzureOpenAI,\n",
    "    chunks: list[str], \n",
    "    chunk: str, \n",
    "    x: int = 5, \n",
    "    num_distract: int = 3, \n",
    "    p: float = 0.8,\n",
    "    model: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Given a chunk, create {Q, A, D} triplets and add them to the dataset.\n",
    "\n",
    "     Parameters:  \n",
    "    client (AzureOpenAI): An instance of the Azure OpenAI client used to communicate with the OpenAI API.  \n",
    "    chunks (list[str]): A list of chunks of text from which distractor documents can be sampled.  \n",
    "    chunk (str): The chunk of text to use as the primary context for generating questions and answers.  \n",
    "    x (int, optional): The number of questions to generate for the given chunk. Default is 5.  \n",
    "    num_distract (int, optional): The number of distractor documents to include with each question. Default is 3.  \n",
    "    p (float, optional): The probability of including the oracle (original) document as part of the context. Default is 0.8.  \n",
    "    model (str, optional): The specific model to use for generating questions and answers. Default is None, which uses the default model configured in the client. \n",
    "    \"\"\"\n",
    "    global ds\n",
    "    global errors\n",
    "    i = chunks.index(chunk)\n",
    "    try:\n",
    "        qs = generate_instructions_gen(client, chunk, x, model)\n",
    "    except Exception as e:\n",
    "        errors.append(e)\n",
    "        return None\n",
    "    for q in qs:\n",
    "        datapt = {\n",
    "            \"id\": None,\n",
    "            \"type\": None,\n",
    "            \"question\": None,\n",
    "            \"context\": None,\n",
    "            \"oracle_context\": None,\n",
    "            \"cot_answer\": None\n",
    "        }\n",
    "\n",
    "        datapt[\"id\"] = f\"seed_task_{i}\"\n",
    "        datapt[\"type\"] = \"general\"\n",
    "        datapt[\"question\"] = q\n",
    "\n",
    "        # add num_distract distractor docs\n",
    "        docs = [chunk]\n",
    "        indices = list(range(0, len(chunks)))\n",
    "        indices.remove(i)\n",
    "        for j in random.sample(indices, num_distract):\n",
    "            docs.append(chunks[j])\n",
    "        \n",
    "        # decides whether to add oracle document\n",
    "        oracle = random.uniform(0, 1) < p\n",
    "        if not oracle:\n",
    "            docs[0] = chunks[random.sample(indices, 1)[0]]\n",
    "        random.shuffle(docs)\n",
    "\n",
    "        d = {\n",
    "            \"title\": [],\n",
    "            \"sentences\": []\n",
    "        }\n",
    "\n",
    "        d[\"title\"].append([\"placeholder_title\"]*(num_distract+1))\n",
    "        d[\"sentences\"].append(docs)\n",
    "        datapt[\"context\"] = d\n",
    "        datapt[\"oracle_context\"] = chunk\n",
    "\n",
    "        # add answer to q\n",
    "        try:\n",
    "            datapt[\"cot_answer\"] = generate_label(client, q, chunk, model=model)\n",
    "        except Exception as e:\n",
    "            errors.append(e)\n",
    "            continue\n",
    "\n",
    "        # construct model instruction \n",
    "        context = \"\"\n",
    "        for doc in docs:\n",
    "            context += \"<DOCUMENT>\" + str(doc) + \"</DOCUMENT>\\n\"\n",
    "        context += q\n",
    "        datapt[\"instruction\"] = context\n",
    "\n",
    "        # add to dataset\n",
    "        if not ds:\n",
    "            # init ds\n",
    "            datapt[\"id\"] = [datapt[\"id\"]]\n",
    "            datapt[\"type\"] = [datapt[\"type\"]]\n",
    "            datapt[\"question\"] = [datapt[\"question\"]]\n",
    "            datapt[\"context\"] = [datapt[\"context\"]]\n",
    "            datapt[\"oracle_context\"] = [datapt[\"oracle_context\"]]\n",
    "            datapt[\"cot_answer\"] = [datapt[\"cot_answer\"]]\n",
    "            datapt[\"instruction\"] = [datapt[\"instruction\"]]\n",
    "            ds = Dataset.from_dict(datapt)\n",
    "        else:\n",
    "            ds = ds.add_item(datapt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's execute this function a in multi-threaded way to speed up the process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "errors = []\n",
    "ds = Dataset.from_dict({})\n",
    "\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    add_chunk_to_dataset(generator_client, chunks, chunk, 5, 3, model=gpt4o_deployment)\n",
    "\n",
    "# Create a ThreadPoolExecutor with the desired number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the tasks to the executor and store the Future objects\n",
    "    futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "\n",
    "    # Use tqdm to create a progress bar\n",
    "    with tqdm(total=len(chunks), desc=\"Processing chunks\") as pbar:\n",
    "        # Iterate over the completed futures as they become available\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            # Get the result of the completed future\n",
    "            result = future.result()\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "# Print any errors that occurred during processing\n",
    "print(f'Number of processing errors: {errors}/{len(chunks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = ds.to_pandas()\n",
    "\n",
    "print(f'{training_df.shape[0]} rows and {training_df.shape[1]} columns in the training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the generated data\n",
    "\n",
    "training_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Formatting the data in chat format for fine tuning with Azure OpenAI**\n",
    "\n",
    "The conversational chat format is required to fine-tune gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df[\"messages\"] = training_df.apply(lambda x: [\n",
    "                                                     {\"role\":\"user\", \"content\":x['instruction']},\n",
    "                                                     {\"role\":\"assistant\", \"content\":x['cot_answer']}\n",
    "                                                     ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.messages.values[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.dropna(subset=['cot_answer'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spitting our data into training and test sets\n",
    "\n",
    "Splitting your data into training, validation and testing sets when fine-tuning a large language model (LLM) is crucial for ensuring the model's performance and generalization capabilities. The training set is used to teach the model, allowing it to learn patterns from the data. The validation set is used to track performance metrics during the training to avoid underfitting / overfitting. However, to objectively evaluate how well the model has learned and to ensure it can generalize to unseen data, a separate testing set is necessary. \n",
    "\n",
    "We will use this test set in order to measure the improvement of performance we get from using RAFT over RAG with gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "train_df, validate_df, test_df = np.split(\n",
    "    training_df.sample(frac=1, random_state=42), \n",
    "                       [int(.8*len(training_df)), int(.9*len(training_df))]\n",
    "                       )\n",
    "\n",
    "print(f\"Train: {train_df.shape[0]}, Validate: {validate_df.shape[0]}, Test: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(\"./data/training_data\"):\n",
    "    os.makedirs(\"./data/training_data\")\n",
    "\n",
    "train_df[['messages']].to_json(\"./data/training_data/banking_train.jsonl\", orient=\"records\", lines=True)\n",
    "test_df.to_json(\"./data/training_data/banking_test.jsonl\", orient=\"records\", lines=True)\n",
    "validate_df[['messages']].to_json(\"./data/training_data/banking_validation.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congrats! We now have a labelled training dataset and a test dataset to evaluate our model's performance. Now go to the [finetuning notebook](./raft_finetuning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
