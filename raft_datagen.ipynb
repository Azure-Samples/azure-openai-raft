{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RAFT training data generation using GPT-4o\n",
    "\n",
    "In this notebook, we'll synthesize some training data that will eventually be used to fine-tune a GPT-4o mini model in order to adapt it to a set of document(s) and specific domain. **This step is critical as the quality of your training data will greatly influence the performance of your fine-tuned model.**\n",
    "\n",
    "For RAFT, these are the different steps to prepare the training dataset:\n",
    "- Collect Domain-Specific Documents: Gather documents relevant to the domain you want to specialize the LLM in (e.g., medical documents for PubMed, legal documents, API documentation for software).\n",
    "- Chunk the file into Documents\n",
    "- For each Document chunk, generate a set of Questions that can be answered from the Document\n",
    "- For each Document-Question pair, create a list of documents using:\n",
    "    - **Golden Document (D*)**: Document that contains the answer to the question.\n",
    "    - **Distractor Documents (Dk)**: Documents that do not contain relevant information.\n",
    "- Question-Answer-Document Triplets: From each **Document-Question** pair, generate a factual **Answer** based on the Golden Document.\n",
    "\n",
    "Curating a good training dataset often involves manual work and review by SMEs. That said, we can use an LLM to help us generate an initial set of training examples that can be vetted and further refined by SMEs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Pre-requisites\n",
    "\n",
    "1. Create a code environment and install the necessary packages\n",
    "\n",
    "```shell\n",
    "conda env create -n raft python=3.10\n",
    "\n",
    "conda activate raft\n",
    "\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "2. Create a .env file based on the sample.env file in this repository. Note: You'll need a GPT-4o and a GPT-4o mini deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from typing import Literal, Any\n",
    "import os\n",
    "from math import ceil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and chunking domain-specific documents\n",
    "\n",
    "For Retrieval Augmented Fine Tuning, we need to generate Question-Documents-Answer triplets. The first step is to create document chunks based on our domain-specific documents we want to specialize our model on.\n",
    "\n",
    "For this workshop, we will use the publicly available [BMO Better Banking Guide](./data/better_banking_guide_en.pdf)\n",
    "\n",
    "The guide contains information about various banking accounts offered by BMO as well as how-to guides on e.g how to access Account statements etc.\n",
    "\n",
    "The document is in PDF format and contains a number of tables and charts, we will use GPT-4o to convert the pages content to markdown.\n",
    "\n",
    "**a. First we'll need to convert the document pages to images encoded in base64**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_base64_urls(pdf_path):\n",
    "  \"\"\"Converts each page of a PDF to a base64 encoded URL starting with 'data:image/jpeg'.\n",
    "\n",
    "  Args:\n",
    "    pdf_path: Path to the PDF file.\n",
    "\n",
    "  Returns:\n",
    "    A list of base64 encoded image URLs, one for each page.\n",
    "  \"\"\"\n",
    "\n",
    "  images = convert_from_path(pdf_path)\n",
    "  base64_urls = []\n",
    "\n",
    "  for image in images:\n",
    "    img_byte_arr = BytesIO()\n",
    "    image.save(img_byte_arr, format=\"JPEG\")\n",
    "    img_byte_arr.seek(0)\n",
    "    base64_encoded = base64.b64encode(img_byte_arr.read()).decode('utf-8')\n",
    "    base64_url = f\"data:image/jpeg;base64,{base64_encoded}\"\n",
    "    base64_urls.append(base64_url)\n",
    "\n",
    "  return base64_urls\n",
    "\n",
    "pdf_path = 'data/better_banking_guide_en.pdf'\n",
    "image_data = pdf_to_base64_urls(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Now we can call GPT-4o to convert our images to markdown**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting images to Markdown: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [04:34<00:00, 10.54s/it]\n"
     ]
    }
   ],
   "source": [
    "def gpt_image_to_markdown(image_data, client):\n",
    "    \"\"\"\n",
    "    Converts an image to markdown using GPT-4o.\n",
    "    \n",
    "    Args:\n",
    "    image_data: A base64 encoded image.\n",
    "    client: An AzureOpenAI client\n",
    "\n",
    "    Returns:\n",
    "    A list of base64 encoded image URLs, one for each page.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\":\"\"\"You are an AI image assistant capable of extracting text from images\n",
    "         Given an image, you must extract the any visible text on the image and return it in Markdown.\n",
    "         You must keep the original layout and formatting of the text as much as possible in Markdown format.\n",
    "         Pay attention to the text size and use headers, subheaders, bold, italic, tables etc where necessary.\"\"\"},\n",
    "         {\"role\":\"user\", \"content\":[{\n",
    "                \"type\":\"image_url\",\n",
    "                \"image_url\":{\n",
    "                    \"url\":image_data\n",
    "                    }\n",
    "         }]}\n",
    "    ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-global\",\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_API_ENDPOINT\"),\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "markdown_doc = \"\"\n",
    "\n",
    "with tqdm(total=len(image_data), desc=\"Converting images to Markdown\") as pbar:\n",
    "    for img_data in image_data:\n",
    "        result = gpt_image_to_markdown(img_data, client)\n",
    "        markdown_doc += \"\\n\" + result\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Let's chunk the Markdown using Langchain**\n",
    "\n",
    "We first use Langchain's `MarkdownHeaderTextSplitter` to split the document based on headers and then further split the chunks using `RecursiveCharacterTextSplitter` with a chunk size of 1024. Finally, we filter out any chunks that are too short to contain any valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks after markdown split: 14\n",
      "Number of chunks after markdown + recursive split: 107\n",
      "Number of chunks after filtering out empty: 100\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "def remove_special_characters(string):\n",
    "    \"\"\"\n",
    "    Remove special characters from a string.\n",
    "    \n",
    "    Parameters:  \n",
    "    string (str): The input string from which special characters need to be removed.  \n",
    "  \n",
    "    Returns:  \n",
    "    str: A new string with special characters removed.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', string)\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\")    \n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=False\n",
    "    )\n",
    "\n",
    "markdown_doc_splits = markdown_splitter.split_text(markdown_doc)\n",
    "print(f\"Number of chunks after markdown split: {len(markdown_doc_splits)}\")\n",
    "\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "\n",
    "chunked_document = text_splitter.split_documents(markdown_doc_splits)\n",
    "\n",
    "print(f\"Number of chunks after markdown + recursive split: {len(chunked_document)}\")\n",
    "\n",
    "chunks = [chunk.page_content for chunk in chunked_document if len(remove_special_characters(chunk.page_content))>100]\n",
    "\n",
    "print(f\"Number of chunks after filtering out empty: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate training data from the chunked documents\n",
    "\n",
    "We define 2 main functions to generate our Question-Document-Answer triplets from our chunked document\n",
    "\n",
    "1. `generate_instructions_gen()`: This function generates a list of questions based on an input document chunk\n",
    "2. `generate_label()`: This function generates an Answer based on a Question-Document chunk pair\n",
    "\n",
    "**a. First, lets look at the `generate_instructions_gen()` function on a sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_str(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function for helping format strings returned by GPT-4o.\n",
    "    \n",
    "    Parameters:  \n",
    "    s (str): The input string to be formatted.  \n",
    "  \n",
    "    Returns:  \n",
    "    str: A formatted string \n",
    "    \"\"\"\n",
    "    l, r = 0, len(s)-1\n",
    "    beg_found = False\n",
    "    for i in range(len(s)):\n",
    "        if s[i].isalpha():\n",
    "            if not beg_found:\n",
    "                l = i\n",
    "                beg_found = True\n",
    "            else:\n",
    "                r = i \n",
    "    r += 2\n",
    "    return s[l:min(r, len(s))]\n",
    "\n",
    "def generate_instructions_gen(client: AzureOpenAI, chunk: Any, x: int = 5, model: str = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates a list of questions or use cases based on a provided chunk of context using an Azure OpenAI model.  \n",
    "\n",
    "    Parameters:  \n",
    "    client (AzureOpenAI): An instance of the Azure OpenAI client used to communicate with the OpenAI API.  \n",
    "    chunk (Any): The context or chunk of text based on which the questions are to be generated.  \n",
    "    x (int, optional): The number of questions to generate. Default is 5.  \n",
    "    model (str, optional): The specific model to use for generating the questions. Default is None, which uses the default model configured in the client.  \n",
    "  \n",
    "    Returns:  \n",
    "    list[str]: A list of generated questions.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a synthetic question-answer pair generator. Given a chunk of context about some topic(s), generate exactly %s example questions a user could ask and would be answered using information from the chunk. For example, if the given context was a Wikipedia paragraph about the United States, an example question could be 'How many states are in the United States?'\" % (x)},\n",
    "            {\"role\": \"system\", \"content\": \"The questions should be able to be answered in a few words or less. Include only the questions in your response.\"},\n",
    "            {\"role\": \"user\", \"content\": str(chunk)}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    queries = response.choices[0].message.content.split('\\n')\n",
    "    queries = [strip_str(q) for q in queries]\n",
    "    queries = [q for q in queries if any(c.isalpha() for c in q)]\n",
    "    return queries[:int(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize an example picked randomly from our Document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis_model = os.getenv(\"AZURE_OPENAI_4o_DEPLOYMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = random.randint(0, len(chunks)-1)\n",
    "chunk = chunks[sample_index]\n",
    "\n",
    "queries = generate_instructions_gen(client, chunk, x=5, model=\"gpt-4o-global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Services                                    | Primary Chequing | Interest Chequingâ´ | Premium Rate Savings | Smart Saverâ´ |\n",
      "|---------------------------------------------|------------------|--------------------|----------------------|--------------|\n",
      "| **Self-serve**                              |                  |                    |                      |              |\n",
      "| **Electronic (per item)**                   |                  |                    |                      |              |\n",
      "| Pre-authorized bill payment/debit           | $1.00            | $1.00              | $1.00                | $5.00âµ       |\n",
      "| Debit card purchaseÂ¹                        | $1.00            | $1.00              | Not available        | Not available|\n",
      "| Interac Online debit                        | $1.00            | $1.00              | $1.00                | Not available|\n",
      "| **ABM (per item)**                          |                  |                    |                      |              |\n"
     ]
    }
   ],
   "source": [
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How much does a pre-authorized bill payment/debit cost with a Primary Chequing account?',\n",
       " 'What is the cost of a debit card purchase with an Interest Chequing account?',\n",
       " 'Are debit card purchases available with a Premium Rate Savings account?',\n",
       " 'How much does an Interac Online debit cost with a Smart Saver account?',\n",
       " 'What is the fee for a pre-authorized bill payment/debit with a Smart Saver account?']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Generating questions, answers and adding distractor documents** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, load_dataset\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "def encode_question_gen(question: str, chunk: Any) -> list[str]:\n",
    "    \"\"\"\n",
    "    Encode multiple prompt instructions into a single string for the general case (`pdf`, `json`, or `txt`).\n",
    "\n",
    "    Parameters:  \n",
    "    question (str): The question to be answered.  \n",
    "    chunk (Any): The context or chunk of text that provides the information needed to answer the question.  \n",
    "  \n",
    "    Returns:  \n",
    "    list[str]: A list of messages formatted for the language model API, including system and user roles.  \n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = []\n",
    "        \n",
    "    prompt = \"\"\"\n",
    "        Question: {question}\\n Context: {context}\\n\n",
    "        Answer this question using the information given in the context above and no prior knowledge. Here is things to pay attention to: \n",
    "        - First provide step-by-step reasoning on how to answer the question. \n",
    "        - In the reasoning, if you need to copy paste some sentences from the context, include them in ##begin_quote## and ##end_quote##. This would mean that things outside of ##begin_quote## and ##end_quote## are not directly copy paste from the context. \n",
    "        - End your response with final answer in the form <ANSWER>: $answer, the answer should be given in a joyful and friendly tone.\n",
    "        - If the answer cannot be found in the context, say \"I'm sorry, I cannot answer this question as I'm missing the required information\"\n",
    "        You MUST begin your final answer with the tag \"<ANSWER>:\".\n",
    "    \"\"\".format(question=question, context=str(chunk))\n",
    "    prompts.append({\"role\": \"system\", \"content\": \"You are a helpful question answerer who can provide an answer given a question and relevant context.\"})\n",
    "    prompts.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return prompts\n",
    "\n",
    "def generate_label(client: AzureOpenAI, question: str, context: Any, model: str = None) -> str | None:\n",
    "    \"\"\"\n",
    "    Generates the label / answer to `question` using `context` and GPT-4o.\n",
    "\n",
    "    Parameters:  \n",
    "    client (AzureOpenAI): An instance of the Azure OpenAI client used to communicate with the OpenAI API.  \n",
    "    question (str): The question to be answered.  \n",
    "    context (Any): The context or chunk of text that provides the information needed to answer the question.  \n",
    "    model (str, optional): The specific model to use for generating the answer. Default is None, which uses the default model configured in the client.  \n",
    "  \n",
    "    Returns:  \n",
    "    str | None: The generated answer from the language model, or None if no answer was generated.\n",
    "    \"\"\"\n",
    "    question = encode_question_gen(question, context)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=question,\n",
    "        n=1,\n",
    "        temperature=0\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "def add_chunk_to_dataset(\n",
    "    client: AzureOpenAI,\n",
    "    chunks: list[str], \n",
    "    chunk: str, \n",
    "    x: int = 5, \n",
    "    num_distract: int = 3, \n",
    "    p: float = 0.8,\n",
    "    model: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Given a chunk, create {Q, A, D} triplets and add them to the dataset.\n",
    "\n",
    "     Parameters:  \n",
    "    client (AzureOpenAI): An instance of the Azure OpenAI client used to communicate with the OpenAI API.  \n",
    "    chunks (list[str]): A list of chunks of text from which distractor documents can be sampled.  \n",
    "    chunk (str): The chunk of text to use as the primary context for generating questions and answers.  \n",
    "    x (int, optional): The number of questions to generate for the given chunk. Default is 5.  \n",
    "    num_distract (int, optional): The number of distractor documents to include with each question. Default is 3.  \n",
    "    p (float, optional): The probability of including the oracle (original) document as part of the context. Default is 0.8.  \n",
    "    model (str, optional): The specific model to use for generating questions and answers. Default is None, which uses the default model configured in the client. \n",
    "    \"\"\"\n",
    "    global ds\n",
    "    global errors\n",
    "    i = chunks.index(chunk)\n",
    "    try:\n",
    "        qs = generate_instructions_gen(client, chunk, x, model)\n",
    "    except Exception as e:\n",
    "        errors.append(e)\n",
    "        return None\n",
    "    for q in qs:\n",
    "        datapt = {\n",
    "            \"id\": None,\n",
    "            \"type\": None,\n",
    "            \"question\": None,\n",
    "            \"context\": None,\n",
    "            \"oracle_context\": None,\n",
    "            \"cot_answer\": None\n",
    "        }\n",
    "\n",
    "        datapt[\"id\"] = f\"seed_task_{i}\"\n",
    "        datapt[\"type\"] = \"general\"\n",
    "        datapt[\"question\"] = q\n",
    "\n",
    "        # add num_distract distractor docs\n",
    "        docs = [chunk]\n",
    "        indices = list(range(0, len(chunks)))\n",
    "        indices.remove(i)\n",
    "        for j in random.sample(indices, num_distract):\n",
    "            docs.append(chunks[j])\n",
    "        \n",
    "        # decides whether to add oracle document\n",
    "        oracle = random.uniform(0, 1) < p\n",
    "        if not oracle:\n",
    "            docs[0] = chunks[random.sample(indices, 1)[0]]\n",
    "        random.shuffle(docs)\n",
    "\n",
    "        d = {\n",
    "            \"title\": [],\n",
    "            \"sentences\": []\n",
    "        }\n",
    "\n",
    "        d[\"title\"].append([\"placeholder_title\"]*(num_distract+1))\n",
    "        d[\"sentences\"].append(docs)\n",
    "        datapt[\"context\"] = d\n",
    "        datapt[\"oracle_context\"] = chunk\n",
    "\n",
    "        # add answer to q\n",
    "        try:\n",
    "            datapt[\"cot_answer\"] = generate_label(client, q, chunk, model=model)\n",
    "        except Exception as e:\n",
    "            errors.append(e)\n",
    "            continue\n",
    "\n",
    "        # construct model instruction \n",
    "        context = \"\"\n",
    "        for doc in docs:\n",
    "            context += \"<DOCUMENT>\" + str(doc) + \"</DOCUMENT>\\n\"\n",
    "        context += q\n",
    "        datapt[\"instruction\"] = context\n",
    "\n",
    "        # add to dataset\n",
    "        if not ds:\n",
    "            # init ds\n",
    "            datapt[\"id\"] = [datapt[\"id\"]]\n",
    "            datapt[\"type\"] = [datapt[\"type\"]]\n",
    "            datapt[\"question\"] = [datapt[\"question\"]]\n",
    "            datapt[\"context\"] = [datapt[\"context\"]]\n",
    "            datapt[\"oracle_context\"] = [datapt[\"oracle_context\"]]\n",
    "            datapt[\"cot_answer\"] = [datapt[\"cot_answer\"]]\n",
    "            datapt[\"instruction\"] = [datapt[\"instruction\"]]\n",
    "            ds = Dataset.from_dict(datapt)\n",
    "        else:\n",
    "            ds = ds.add_item(datapt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's execute this function a in multi-threaded way to speed up the process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:20<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processing errors: []/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "errors = []\n",
    "ds = Dataset.from_dict({})\n",
    "\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    add_chunk_to_dataset(client, chunks, chunk, 5, 3, model=\"gpt-4o-global\")\n",
    "\n",
    "# Create a ThreadPoolExecutor with the desired number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the tasks to the executor and store the Future objects\n",
    "    futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "\n",
    "    # Use tqdm to create a progress bar\n",
    "    with tqdm(total=len(chunks), desc=\"Processing chunks\") as pbar:\n",
    "        # Iterate over the completed futures as they become available\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            # Get the result of the completed future\n",
    "            result = future.result()\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "# Print any errors that occurred during processing\n",
    "print(f'Number of processing errors: {errors}/{len(chunks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477 rows and 7 columns in the training dataset\n"
     ]
    }
   ],
   "source": [
    "training_df = ds.to_pandas()\n",
    "\n",
    "print(f'{training_df.shape[0]} rows and {training_df.shape[1]} columns in the training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>oracle_context</th>\n",
       "      <th>cot_answer</th>\n",
       "      <th>instruction</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seed_task_5</td>\n",
       "      <td>general</td>\n",
       "      <td>What is the minimum balance required for a Sma...</td>\n",
       "      <td>{'sentences': [['Â¹ Customer is responsible for...</td>\n",
       "      <td>| is available in U.S. dollars                ...</td>\n",
       "      <td>To determine the minimum balance required for ...</td>\n",
       "      <td>&lt;DOCUMENT&gt;Â¹ Customer is responsible for all fe...</td>\n",
       "      <td>[{'role': 'user', 'content': '&lt;DOCUMENT&gt;Â¹ Cust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seed_task_3</td>\n",
       "      <td>general</td>\n",
       "      <td>How many personal accounts does BMO Bank of Mo...</td>\n",
       "      <td>{'sentences': [['| Per Item | Per Item |\n",
       "| ---...</td>\n",
       "      <td>## Personal accounts  \\n### Step 1: Select you...</td>\n",
       "      <td>To determine how many personal accounts BMO Ba...</td>\n",
       "      <td>&lt;DOCUMENT&gt;| Per Item | Per Item |\\n| --- | ---...</td>\n",
       "      <td>[{'role': 'user', 'content': '&lt;DOCUMENT&gt;| Per ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id     type                                           question  \\\n",
       "0  seed_task_5  general  What is the minimum balance required for a Sma...   \n",
       "1  seed_task_3  general  How many personal accounts does BMO Bank of Mo...   \n",
       "\n",
       "                                             context  \\\n",
       "0  {'sentences': [['Â¹ Customer is responsible for...   \n",
       "1  {'sentences': [['| Per Item | Per Item |\n",
       "| ---...   \n",
       "\n",
       "                                      oracle_context  \\\n",
       "0  | is available in U.S. dollars                ...   \n",
       "1  ## Personal accounts  \\n### Step 1: Select you...   \n",
       "\n",
       "                                          cot_answer  \\\n",
       "0  To determine the minimum balance required for ...   \n",
       "1  To determine how many personal accounts BMO Ba...   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  <DOCUMENT>Â¹ Customer is responsible for all fe...   \n",
       "1  <DOCUMENT>| Per Item | Per Item |\\n| --- | ---...   \n",
       "\n",
       "                                            messages  \n",
       "0  [{'role': 'user', 'content': '<DOCUMENT>Â¹ Cust...  \n",
       "1  [{'role': 'user', 'content': '<DOCUMENT>| Per ...  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previewing the generated data\n",
    "\n",
    "training_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Formatting the data in chat format for fine tuning with Azure OpenAI**\n",
    "\n",
    "The conversational chat format is required to fine-tune gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df[\"messages\"] = training_df.apply(lambda x: [\n",
    "                                                     {\"role\":\"user\", \"content\":x['instruction']},\n",
    "                                                     {\"role\":\"assistant\", \"content\":x['cot_answer']}\n",
    "                                                     ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"<DOCUMENT>You can reduce the impact to the environment!\\n\\nWe offer you the ability to stop receiving paper copies of your Everyday Banking statements for your Personal accounts2. Go to a branch to request the paperless option or, more conveniently, sign into Online Banking, select the Accounts and Plans tab followed by the Bank Statements tab and simply select the option to stop receiving paper copies of your Everyday Banking statements. If you are not already registered for Online Banking, be sure to register at bmo.com today.\\n\\nWhatâ€™s more, you are now able to view your cheque images through Online Banking.\\n\\n---\\n\\n## Account Tracker\\n\\nBMO Account Tracker is a complementary money management service that provides you with a complete picture of your personal finances.\\n\\n---</DOCUMENT>\\n<DOCUMENT>Of course, we're always here to help â€” you can contact us online, by phone or by visiting one of our branches.\\n\\n---\\n\\nÂ¹ Some products and services are only available to BMO customers. Our ability to offer these services is subject to our providers' policies to comply with applicable regulations. In some instances, the use of these services may be restricted and not available to all customers. Â² Based on debit card purchases minus refunds from your Primary Chequing account, with one of the following Everyday Banking Plans: Performance, Premium, Senior Plan with AIR MILES. A maximum of 40 reward miles can be earned per month.\\n```\\n```markdown\\n# Contents\\n\\n## Three steps to Better Banking:\\n\\n### STEP 1\\n#### Select your personal accounts\\n- Primary Chequing\\n- Smart Saver Account\\n- Premium Rate Savings\\n\\n### STEP 2\\n#### Select your Banking Plan\\n- Features of all Banking Plans\\n- How to get free banking\\n\\n### STEP 3\\n#### Get started\\n- Identification requirements\\n- Cashing Government of Canada cheques\\n- Privacy</DOCUMENT>\\n<DOCUMENT>---\\n\\n_Page 5_                                                                                     _6_\\n```  \\n```markdown\\n## Banking Plan features\\n\\n| | Performance Plan | Premium Plan  | Plus Plan | Practical Plan | Senior Plan | Senior Plan with AIR MILES |\\n|---|---|---|---|---|---|---|\\n| **Everyday Banking** | | | | | | |\\n| Monthly transaction limit (including account history inquiries) | Unlimited | Unlimited | 30 | 10 | Unlimited | Unlimited |\\n| Monthly number of debit transactions using non-BMO ABM on the Interac** network | 1 | 10 | 2 | 1 | 1 | 1 |\\n| Monthly number of Interac e-Transfers | 2 | 2 | 1 | 1 | 1 | 1 |\\n| Cheque image return option* | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ |\\n| Assisted-service bill payment at no extra charge | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ |</DOCUMENT>\\n<DOCUMENT>| offers **free access to BMO MoneyLogicâ„¢**                                                                                       | âœ”                | âœ”            | âœ”         | âœ”               |                       |                           |</DOCUMENT>\\nDoes BMO offer free access to MoneyLogic?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'To determine if BMO offers free access to MoneyLogic, we need to carefully examine the provided context for any relevant information.\\n\\n1. Identify the key phrase in the question: \"Does BMO offer free access to MoneyLogic?\"\\n2. Look for any mention of BMO and MoneyLogic in the context.\\n3. Check if the context specifies whether the access to MoneyLogic is free.\\n\\nUpon reviewing the context, we find the following relevant information:\\n\\n##begin_quote##\\noffers **free access to BMO MoneyLogicâ„¢**\\n##end_quote##\\n\\nThis indicates that BMO does indeed offer free access to MoneyLogic.\\n\\n<ANSWER>: Yes, BMO offers free access to MoneyLogic! ðŸŽ‰ Enjoy managing your finances with this handy tool at no cost!'}]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.messages.values[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.dropna(subset=['cot_answer'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spitting our data into training and test sets\n",
    "\n",
    "Splitting your data into training and testing sets when fine-tuning a large language model (LLM) is crucial for ensuring the model's performance and generalization capabilities. The training set is used to teach the model, allowing it to learn patterns from the data. However, to objectively evaluate how well the model has learned and to ensure it can generalize to unseen data, a separate testing set is necessary. \n",
    "\n",
    "We will use this test set in order to measure the improvement of performance we get from using RAFT over RAG with gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(training_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(\"data/training_data\"):\n",
    "    os.makedirs(\"data/training_data\")\n",
    "\n",
    "train_df.to_json(\"data/training_data/banking_train.jsonl\", orient=\"records\", lines=True)\n",
    "test_df.to_json(\"data/training_data/banking_test.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_json(\"data/training_data/banking_test_with_metadata.json\", orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
