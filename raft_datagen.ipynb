{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import PyPDF2\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_API_ENDPOINT\"),\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk the input document \n",
    "data_path = \"data/vampires/Vampire - Wikipedia.pdf\"\n",
    "CHUNK_SIZE = 512\n",
    "embedding_deployment = \"embed\"\n",
    "\n",
    "text=\"\"\n",
    "with open(data_path, 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    num_pages = len(reader.pages)\n",
    "    for page_num in range(num_pages):\n",
    "        page = reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "\n",
    "num_chunks = ceil(len(text) / CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_langchain_embeddings():\n",
    "\n",
    "    embedding_client = AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_API_ENDPOINT\"),\n",
    "        api_version=\"2024-02-01\",\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        deployment=\"embed\"\n",
    "    )\n",
    "\n",
    "    return embedding_client\n",
    "\n",
    "embed_client = build_langchain_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(embed_client, number_of_chunks=CHUNK_SIZE)\n",
    "chunks = text_splitter.create_documents([text])\n",
    "chunks = [chunk.page_content for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Any\n",
    "\n",
    "def strip_str(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function for helping format strings returned by GPT-4.\n",
    "    \"\"\"\n",
    "    l, r = 0, len(s)-1\n",
    "    beg_found = False\n",
    "    for i in range(len(s)):\n",
    "        if s[i].isalpha():\n",
    "            if not beg_found:\n",
    "                l = i\n",
    "                beg_found = True\n",
    "            else:\n",
    "                r = i \n",
    "    r += 2\n",
    "    return s[l:min(r, len(s))]\n",
    "\n",
    "def generate_instructions_gen(client: AzureOpenAI, chunk: Any, x: int = 5, model: str = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates `x` questions / use cases for `chunk`. Used when the input document is of general types \n",
    "    `pdf`, `json`, or `txt`.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a synthetic question-answer pair generator. Given a chunk of context about some topic(s), generate %s example questions a user could ask and would be answered using information from the chunk. For example, if the given context was a Wikipedia paragraph about the United States, an example question could be 'How many states are in the United States?'\" % (x)},\n",
    "            {\"role\": \"system\", \"content\": \"The questions should be able to be answered in a few words or less. Include only the questions in your response.\"},\n",
    "            {\"role\": \"user\", \"content\": str(chunk)}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    queries = response.choices[0].message.content.split('\\n')\n",
    "    queries = [strip_str(q) for q in queries]\n",
    "    queries = [q for q in queries if any(c.isalpha() for c in q)]\n",
    "\n",
    "    return queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = chunks[0]\n",
    "\n",
    "queries = generate_instructions_gen(client, chunk, x=5, model=\"gpt-4o-global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who is the author of \"The Vampire\" from ',\n",
       " 'What does a vampire generally feed on?',\n",
       " 'In which type of folklore are vampires undead humanoid creatures?',\n",
       " 'What kind of activities did vampires engage in according to European folklore?',\n",
       " 'When was \"The Vampire\" by Philip Burne-Jones published?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = queries\n",
    "qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vhoudebine/miniconda3/envs/openai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, load_dataset\n",
    "import random\n",
    "\n",
    "def encode_question_gen(question: str, chunk: Any) -> list[str]:\n",
    "    \"\"\"\n",
    "    Encode multiple prompt instructions into a single string for the general case (`pdf`, `json`, or `txt`).\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = []\n",
    "        \n",
    "    prompt = \"\"\"\n",
    "        Question: {question}\\nContext: {context}\\n\n",
    "        Answer this question using the information given in the context above. Here is things to pay attention to: \n",
    "        - First provide step-by-step reasoning on how to answer the question. \n",
    "        - In the reasoning, if you need to copy paste some sentences from the context, include them in ##begin_quote## and ##end_quote##. This would mean that things outside of ##begin_quote## and ##end_quote## are not directly copy paste from the context. \n",
    "        - End your response with final answer in the form <ANSWER>: $answer, the answer should be succinct.\n",
    "        You MUST begin your final answer with the tag \"<ANSWER>:\".\n",
    "    \"\"\".format(question=question, context=str(chunk))\n",
    "    prompts.append({\"role\": \"system\", \"content\": \"You are a helpful question answerer who can provide an answer given a question and relevant context.\"})\n",
    "    prompts.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return prompts\n",
    "\n",
    "def generate_label(client: AzureOpenAI, question: str, context: Any, doctype: Any = \"pdf\", model: str = None) -> str | None:\n",
    "    \"\"\"\n",
    "    Generates the label / answer to `question` using `context` and GPT-4.\n",
    "    \"\"\"\n",
    "    question = encode_question_gen(question, context)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=question,\n",
    "        n=1,\n",
    "        temperature=0\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "def add_chunk_to_dataset(\n",
    "    client: AzureOpenAI,\n",
    "    chunks: list[str], \n",
    "    chunk: str, \n",
    "    x: int = 5, \n",
    "    num_distract: int = 3, \n",
    "    p: float = 0.8,\n",
    "    model: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Given a chunk, create {Q, A, D} triplets and add them to the dataset.\n",
    "    \"\"\"\n",
    "    global ds\n",
    "    i = chunks.index(chunk)\n",
    "    try:\n",
    "        qs = generate_instructions_gen(client, chunk, x, model)\n",
    "    except:\n",
    "        return None\n",
    "    for q in qs:\n",
    "        datapt = {\n",
    "            \"id\": None,\n",
    "            \"type\": None,\n",
    "            \"question\": None,\n",
    "            \"context\": None,\n",
    "            \"oracle_context\": None,\n",
    "            \"cot_answer\": None\n",
    "        }\n",
    "\n",
    "        datapt[\"id\"] = f\"seed_task_{0 if not ds else ds.num_rows}\"\n",
    "        datapt[\"type\"] = \"general\"\n",
    "        datapt[\"question\"] = q\n",
    "\n",
    "        # add num_distract distractor docs\n",
    "        docs = [chunk]\n",
    "        indices = list(range(0, len(chunks)))\n",
    "        indices.remove(i)\n",
    "        for j in random.sample(indices, num_distract):\n",
    "            docs.append(chunks[j])\n",
    "        \n",
    "        # decides whether to add oracle document\n",
    "        oracle = random.uniform(0, 1) < p\n",
    "        if not oracle:\n",
    "            docs[0] = chunks[random.sample(indices, 1)[0]]\n",
    "        random.shuffle(docs)\n",
    "\n",
    "        d = {\n",
    "            \"title\": [],\n",
    "            \"sentences\": []\n",
    "        }\n",
    "\n",
    "        d[\"title\"].append([\"placeholder_title\"]*(num_distract+1))\n",
    "        d[\"sentences\"].append(docs)\n",
    "        datapt[\"context\"] = d\n",
    "        datapt[\"oracle_context\"] = chunk\n",
    "\n",
    "        # add answer to q\n",
    "        try:\n",
    "            datapt[\"cot_answer\"] = generate_label(client, q, chunk, doctype=\"pdf\", model=model)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # construct model instruction \n",
    "        context = \"\"\n",
    "        for doc in docs:\n",
    "            context += \"<DOCUMENT>\" + str(doc) + \"</DOCUMENT>\\n\"\n",
    "        context += q\n",
    "        datapt[\"instruction\"] = context\n",
    "\n",
    "        # add to dataset\n",
    "        if not ds:\n",
    "            # init ds\n",
    "            datapt[\"id\"] = [datapt[\"id\"]]\n",
    "            datapt[\"type\"] = [datapt[\"type\"]]\n",
    "            datapt[\"question\"] = [datapt[\"question\"]]\n",
    "            datapt[\"context\"] = [datapt[\"context\"]]\n",
    "            datapt[\"oracle_context\"] = [datapt[\"oracle_context\"]]\n",
    "            datapt[\"cot_answer\"] = [datapt[\"cot_answer\"]]\n",
    "            datapt[\"instruction\"] = [datapt[\"instruction\"]]\n",
    "            ds = Dataset.from_dict(datapt)\n",
    "        else:\n",
    "            ds = ds.add_item(datapt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/186\n",
      "Processing chunk 2/186\n",
      "Processing chunk 3/186\n",
      "Processing chunk 4/186\n",
      "Processing chunk 5/186\n",
      "Processing chunk 6/186\n",
      "Processing chunk 7/186\n",
      "Processing chunk 8/186\n",
      "Processing chunk 9/186\n",
      "Processing chunk 10/186\n",
      "Processing chunk 11/186\n",
      "Processing chunk 12/186\n",
      "Processing chunk 13/186\n",
      "Processing chunk 14/186\n",
      "Processing chunk 15/186\n",
      "Processing chunk 16/186\n",
      "Processing chunk 17/186\n",
      "Processing chunk 18/186\n",
      "Processing chunk 19/186\n",
      "Processing chunk 20/186\n",
      "Processing chunk 21/186\n",
      "Processing chunk 22/186\n",
      "Processing chunk 23/186\n",
      "Processing chunk 24/186\n",
      "Processing chunk 25/186\n",
      "Processing chunk 26/186\n",
      "Processing chunk 27/186\n",
      "Processing chunk 28/186\n",
      "Processing chunk 29/186\n",
      "Processing chunk 30/186\n",
      "Processing chunk 31/186\n",
      "Processing chunk 32/186\n",
      "Processing chunk 33/186\n",
      "Processing chunk 34/186\n",
      "Processing chunk 35/186\n",
      "Processing chunk 36/186\n",
      "Processing chunk 37/186\n",
      "Processing chunk 38/186\n",
      "Processing chunk 39/186\n",
      "Processing chunk 40/186\n",
      "Processing chunk 41/186\n",
      "Processing chunk 42/186\n",
      "Processing chunk 43/186\n",
      "Processing chunk 44/186\n",
      "Processing chunk 45/186\n",
      "Processing chunk 46/186\n",
      "Processing chunk 47/186\n",
      "Processing chunk 48/186\n",
      "Processing chunk 49/186\n",
      "Processing chunk 50/186\n",
      "Processing chunk 51/186\n",
      "Processing chunk 52/186\n",
      "Processing chunk 53/186\n",
      "Processing chunk 54/186\n",
      "Processing chunk 55/186\n",
      "Processing chunk 56/186\n",
      "Processing chunk 57/186\n",
      "Processing chunk 58/186\n",
      "Processing chunk 59/186\n",
      "Processing chunk 60/186\n",
      "Processing chunk 61/186\n",
      "Processing chunk 62/186\n",
      "Processing chunk 63/186\n",
      "Processing chunk 64/186\n",
      "Processing chunk 65/186\n",
      "Processing chunk 66/186\n",
      "Processing chunk 67/186\n",
      "Processing chunk 68/186\n",
      "Processing chunk 69/186\n",
      "Processing chunk 70/186\n",
      "Processing chunk 71/186\n",
      "Processing chunk 72/186\n",
      "Processing chunk 73/186\n",
      "Processing chunk 74/186\n",
      "Processing chunk 75/186\n",
      "Processing chunk 76/186\n",
      "Processing chunk 77/186\n",
      "Processing chunk 78/186\n",
      "Processing chunk 79/186\n",
      "Processing chunk 80/186\n",
      "Processing chunk 81/186\n",
      "Processing chunk 82/186\n",
      "Processing chunk 83/186\n",
      "Processing chunk 84/186\n",
      "Processing chunk 85/186\n",
      "Processing chunk 86/186\n",
      "Processing chunk 87/186\n",
      "Processing chunk 88/186\n",
      "Processing chunk 89/186\n",
      "Processing chunk 90/186\n",
      "Processing chunk 91/186\n",
      "Processing chunk 92/186\n",
      "Processing chunk 93/186\n",
      "Processing chunk 94/186\n",
      "Processing chunk 95/186\n",
      "Processing chunk 96/186\n",
      "Processing chunk 97/186\n",
      "Processing chunk 98/186\n",
      "Processing chunk 99/186\n",
      "Processing chunk 100/186\n",
      "Processing chunk 101/186\n",
      "Processing chunk 102/186\n",
      "Processing chunk 103/186\n",
      "Processing chunk 104/186\n",
      "Processing chunk 105/186\n",
      "Processing chunk 106/186\n",
      "Processing chunk 107/186\n",
      "Processing chunk 108/186\n",
      "Processing chunk 109/186\n",
      "Processing chunk 110/186\n",
      "Processing chunk 111/186\n",
      "Processing chunk 112/186\n",
      "Processing chunk 113/186\n",
      "Processing chunk 114/186\n",
      "Processing chunk 115/186\n",
      "Processing chunk 116/186\n",
      "Processing chunk 117/186\n",
      "Processing chunk 118/186\n",
      "Processing chunk 119/186\n",
      "Processing chunk 120/186\n",
      "Processing chunk 121/186\n",
      "Processing chunk 122/186\n",
      "Processing chunk 123/186\n",
      "Processing chunk 124/186\n",
      "Processing chunk 125/186\n",
      "Processing chunk 126/186\n",
      "Processing chunk 127/186\n",
      "Processing chunk 128/186\n",
      "Processing chunk 129/186\n",
      "Processing chunk 130/186\n",
      "Processing chunk 131/186\n",
      "Processing chunk 132/186\n",
      "Processing chunk 133/186\n",
      "Processing chunk 134/186\n",
      "Processing chunk 135/186\n",
      "Processing chunk 136/186\n",
      "Processing chunk 137/186\n",
      "Processing chunk 138/186\n",
      "Processing chunk 139/186\n",
      "Processing chunk 140/186\n",
      "Processing chunk 141/186\n",
      "Processing chunk 142/186\n",
      "Processing chunk 143/186\n",
      "Processing chunk 144/186\n",
      "Processing chunk 145/186\n",
      "Processing chunk 146/186\n",
      "Processing chunk 147/186\n",
      "Processing chunk 148/186\n",
      "Processing chunk 149/186\n",
      "Processing chunk 150/186\n",
      "Processing chunk 151/186\n",
      "Processing chunk 152/186\n",
      "Processing chunk 153/186\n",
      "Processing chunk 154/186\n",
      "Processing chunk 155/186\n",
      "Processing chunk 156/186\n",
      "Processing chunk 157/186\n",
      "Processing chunk 158/186\n",
      "Processing chunk 159/186\n",
      "Processing chunk 160/186\n",
      "Processing chunk 161/186\n",
      "Processing chunk 162/186\n",
      "Processing chunk 163/186\n",
      "Processing chunk 164/186\n",
      "Processing chunk 165/186\n",
      "Processing chunk 166/186\n",
      "Processing chunk 167/186\n",
      "Processing chunk 168/186\n",
      "Processing chunk 169/186\n",
      "Processing chunk 170/186\n",
      "Processing chunk 171/186\n",
      "Processing chunk 172/186\n",
      "Processing chunk 173/186\n",
      "Processing chunk 174/186\n",
      "Processing chunk 175/186\n",
      "Processing chunk 176/186\n",
      "Processing chunk 177/186\n",
      "Processing chunk 178/186\n",
      "Processing chunk 179/186\n",
      "Processing chunk 180/186\n",
      "Processing chunk 181/186\n",
      "Processing chunk 182/186\n",
      "Processing chunk 183/186\n",
      "Processing chunk 184/186\n",
      "Processing chunk 185/186\n",
      "Processing chunk 186/186\n"
     ]
    }
   ],
   "source": [
    "ds = None\n",
    "for i in range(len(chunks)):\n",
    "    chunk = chunks[i]\n",
    "    print(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
    "    add_chunk_to_dataset(client, chunks, chunk, \"pdf\", 5, 3, model=\"gpt-4o-global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df[\"messages\"] = training_df.apply(lambda x: [\n",
    "                                                     {\"role\":\"user\", \"content\":x['instruction']},\n",
    "                                                     {\"role\":\"assistant\", \"content\":x['cot_answer']}\n",
    "                                                     ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '<DOCUMENT>In somecases, especially in small localities, beliefs are still rampant and sightings or claims of vampire attacksoccur frequently.</DOCUMENT>\\n<DOCUMENT>She appeared as an attractivewoman with long black hair that covered a hole in the back of her neck, with which she sucked theblood of children.</DOCUMENT>\\n<DOCUMENT>Vampire were released during the jiangshi cinematic boom of the 1980s and1990s.[116][117]In modern fiction, the vampire tends to be depicted as a suave, charismatic villain.[22] Vampirehunting societies still exist, but they are largely formed for social reasons.[20] Allegations of vampireattacks swept through Malawi during late 2002 and early 2003, with mobs stoning one person todeath and attacking at least four others, including Governor Eric Chiwaya, based on the belief that thegovernment was colluding with vampires.[118] Fears and violence recurred in late 2017, with 6 peopleaccused of being vampires killed.[119]In early 1970, local press spread rumours that a vampire hauntedHighgate Cemetery in London.</DOCUMENT>\\n<DOCUMENT>The Vampire, by Philip Burne-Jones, 1897\\nVampireA vampire is a mythical creature that subsists by feeding on thevital essence (generally in the form of blood) of the living. InEuropean folklore, vampires are undead humanoid creatures thatoften visited loved ones and caused mischief or deaths in theneighbourhoods which they inhabited while they were alive.</DOCUMENT>\\n<DOCUMENT>Title page of treatise on thechewing and smacking ofthe dead in graves (1734),a book on vampirology byMichael Ranft.The Old Norse draugr is another medieval example of an undeadcreature with similarities to vampires.[75] Vampiric beings were rarelywritten about in Jewish literature; the 16th-century rabbi David benSolomon ibn Abi Zimra (Radbaz) wrote of an uncharitable old womanwhose body was unguarded and unburied for three days after she diedand rose as a vampiric entity, killing hundreds of people.</DOCUMENT>\\n<DOCUMENT>Vampires had already been discussed in French[16] and German literature.[17]b.</DOCUMENT>\\nWho painted \"The Vampire\" in '},\n",
       " {'role': 'assistant',\n",
       "  'content': 'To answer the question \"Who painted \\'The Vampire\\'?\" using the provided context, follow these steps:\\n\\n1. Identify the relevant information in the context that pertains to the painting \"The Vampire.\"\\n2. Look for the name of the artist associated with the painting.\\n3. Ensure the information is directly related to the painting and not to the general description of vampires.\\n\\nStep-by-step reasoning:\\n\\n1. The context mentions a painting titled \"The Vampire.\"\\n2. The context provides the name of the artist and the year the painting was created.\\n3. The relevant sentence in the context is: ##begin_quote## The Vampire, by Philip Burne-Jones, 1897 ##end_quote##.\\n4. From this sentence, it is clear that Philip Burne-Jones is the artist who painted \"The Vampire.\"\\n\\n<ANSWER>: Philip Burne-Jones'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.messages.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.dropna(subset=['cot_answer'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(training_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/training_data\"):\n",
    "    os.makedirs(\"data/training_data\")\n",
    "train_df[['messages']].to_json(\"data/training_data/vampires_train.jsonl\", orient=\"records\", lines=True)\n",
    "test_df[['messages']].to_json(\"data/training_data/vampires_test.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_json(\"data/training_data/vampires_test_with_metadata.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
